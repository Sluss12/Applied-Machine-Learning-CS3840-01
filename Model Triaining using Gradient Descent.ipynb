{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa20c073",
   "metadata": {},
   "source": [
    "# Create dataset for linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b672a31d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = 2 * np.random.rand(100, 1) #X are 100 inputs with 1-dimensional feature randomly distributed in [0, 2)\n",
    "y = 3 * X + np.random.rand(100, 1) #y are true label values\n",
    "\n",
    "plt.plot(X, y, 'b.')\n",
    "plt.xlabel(\"X\", fontsize=18)\n",
    "plt.ylabel(\"y\", fontsize=14, rotation=0)\n",
    "plt.axis([0, 2, 0, 10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfd424a",
   "metadata": {},
   "source": [
    "# Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b10cb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.1 #Learning rate\n",
    "n_iter = 1000 #Number of iterations for weight updates\n",
    "d_train = 100 #Number of training samples\n",
    "\n",
    "w = np.random.rand(1) #Weight vector\n",
    "\n",
    "for i in range(n_iter):\n",
    "    gradient = 2/d_train * (w.dot(X.T) - y.T).dot(X)\n",
    "    w = w - eta * gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59c358d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.36654517]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42e26414",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = X.dot(w.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45859903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X, y, \"b.\")\n",
    "plt.plot(X, y_predict, \"r-\")\n",
    "plt.xlabel(\"X\", fontsize=18)\n",
    "plt.ylabel(\"y\", fontsize=14, rotation=0)\n",
    "plt.axis([0, 2, 0, 10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858fb0e4",
   "metadata": {},
   "source": [
    "# Impact of learning rate eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "917be0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gradient_descent(w, eta):\n",
    "    plt.plot(X, y, 'b.')\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        if i < 10:\n",
    "            y_predict = X.dot(w.T)\n",
    "            plt.plot(X, y_predict, 'r-')\n",
    "        gradient = 2/d_train * (w.dot(X.T) - y.T).dot(X)\n",
    "        w = w - eta * gradient\n",
    "    \n",
    "    plt.xlabel(\"X\", fontsize=18)\n",
    "    plt.title(\"$\\eta = {}$\".format(eta), fontsize=14, rotation=0)\n",
    "    plt.axis([0, 2, 0, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d418c1ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAs8AAADqCAYAAABdjKgbAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAABNtElEQVR4nO2deZwUxfnGn3eXZVnAC8RbIt4ajRCQsMYDRXMZj0RjNCZgNALeJjEqGiPGBIyJBo/IoRIh8SCKGk3UGJH9qWHwBA/UeKDiibJquPes3x81Tff0dM909/Q583w/n/nsTE93Ve3M0zVPV7/1liilQAghhBBCCClPXdINIIQQQgghJCvQPBNCCCGEEOIRmmdCCCGEEEI8QvNMCCGEEEKIR2ieCSGEEEII8QjNMyGEEEIIIR6heSaEEEIIIcQjNM+EEEIIIYR4hOY5Y4jI6SLyloisF5FnReSASo8RkQki8rSIrBSRT0TkfhHZK7r/ghD/WhaRA0XkPhF5X0SUiJwUU1MJKSCAdifmNWt9fBRXewkpBftW/9A8ZwgR+T6AawBMAjAEwAIAD4rIwAqPGQngBgD7ATgEQCeAR0SkXwT/BiGBtAygL4CXAJwDYF3kjSTEgYDaBYD/Atja8tg7ynYS4gP2rT4RLs8dDiKyGMDfAfQDcAKAbgDXKaUuD7GOJwG8oJQ61bLtdQB3KaUmhHhMXwD/A3C0Uur+sNpPskFatWw7fjWAM5VSt4TVJpJ90qpdEZkI4FilFO/oEV/EoWlbfexbPcCR5xAQkQYAewD4IYCnAIwAMBXAZSKylW3fi0RkdZlH0S1AEekJYCiAh21vPQw9YuzULt/H5NkIWhufldiHVCFp1TIh5ciAdncUkQ/y4R53iMiOAf9VUiPEoWkSjB5JN6BK2BNATwA/V0rdCwAichOAXwHYFIA1tm0agL+VKe99h22bA6gHsNy2fTmAQ13KCXIMoG9JLgaQK9NOUn2kVcuElCPN2n0SwEkAXgWwBYBfAlggIl9USrWWaQepXeLQNAkAzXM47APgUwDWEIcB+b8Fk0KUUp/m900lInI1gP0B7K+U6kq6PSR2qkbLpOZIrXaVUg9aX4vIQgBLAYwBcHVc7SCZI7WarnUYthEOgwE8azObgwG8o5T63LpjBbdWVgDoArClbfuWsJ1EQY8RkT9Cx1QdopRa6lImqW4GI51aJqQcg5ER7SqlVgNYAmAXr8eQmmQwotc0CQBHnsNhHwDP2LYNgQ59sBPo1opSql1EngVwGIA7LW8dBmCuUyF+jhGRawB8H8DBSqlXy7SPVC+p1DIhHsiMdkWkF4DdAcz3egypSSLXNAkGzXM47APgRtu2IQD+bd+xwlsrVwP4i4g8BeA/AMYD2Ab6pIGInAk9S3Z3r8fkj/sTgB8BOBrAZ5aJCKvzIySkdkitlkVngdk5/7IOwEARGQzgU6XUsoDtINVDmrX7B+hb78ugY54vAdAHwKyAbSC1QSyaZt8aAKUUHxU8AGwHQAHY3bJNAKwCcGQE9Z0O4G0AbQCeBXCg5b2J+iv1fkz+feXymJj058tHfI+0axk6H7mTTm9J+rPjI9lHBrR7B4APALRDj/7NBbBn0p8bH+l9xKlp9q3+H8zzHAEisit0QvyBSql3k24PIUGhlklWoXZJtUFNpwdOGIyGIQBaKW5SBVDLJKtQu6TaoKZTQsXmWURmisjHIvKSZVs/Efm3iLye/7tZpfVkDLeAfpIiqF1PUMsphNr1BLWbMqjbiqGmU0LFYRsiciCA1QBmq/zSoyJyJXSg+RUiciGAzZRSF1TcWkJChNolWYXaJVmEuiXVQigxzyKyA4B/WE6G/wIYqZT6UES2BtCilNqt4ooICRlql2QVapdkEeqWVANRxTxvqZT6MP/8IxQnlCckrVC7JKtQuySLULckc0Se51kppUTEdXhbRMYCGAsAffr0Gbr77ru77UqIJ5599tkVSqkB5fcsDbVL4iQs3QKltUvdkrBJrM9ds0a/MXSotTHmc+v2IIRZVtJU0/8SIkG1G5V5Xi4iW1tuw3zstqNSagaAGQAwbNgw9cwz9sV0CPGHiLxTweHULkmECnULeNQudUvCJrE+1zCEVg2LmM8r1XaYZSVNNf0vIRJUu1GFbdwHYEz++RgAf4+oHkLChtolWYXaJVkkGt0eeWQoxVQFVuPMtT1CIYxUdbcDyAHYTUTeE5FTAFwB4DAReR3AofnXhKQKapdkFWqXZJFYdTtzZijFAAAWLgyvLFIVVBy2oZQ6weWtUZWWTUiUULskq1C7JIvEqtv+/cMr6ytfCa+suOGocyRwhUFCCCGE+OPii7Uxe+65pFtCSOxEnm2DEEIIIVXE9tsD772nn29WpQsCWkdsswpHnSODI8+EEEII8caPf2waZwAYNCi5thCSEDTPhBBCCCnPjBnALbeYrzmamV446hwpNM+EEEIIKc1ttwHjxpmva8WQzZmTdAtICqF5JoQQQog7r74KnHii+bpWjDMAHHdc0i3wD0edI4fmmRBCCCHOrF4N7LGH+ZpmjBCaZ0IIIYS4sNFG5vNaMc5ZzrTBUedYoHkmhBBCSDFWI9bZmVw7nDCMYQ9m3CXxQ/NMCCGEkEKsxvmDD4D6+uTa4oRhnnv2NLe99VYybUkLHHWODZpnQgghhJhYTdisWcDWWyfXFje6u/Vfq3k+/vhw6/j1r8Mtj1QNNM+EEEII0ViN82GHAaNHJ9eWUjiNPD/1VLh1XHJJuOVFCUedY4XmmRBCCCHFE+UefjiZdnjByTwTEhM0z4QQQkitYzfOaR+9jMo8ZzHTxoQJ5nPr0ukkMmieCSGEkFoma8YZcI55rlWuuMJ8vu22ybWjhqB5JoQQQmqVLBpngGEbBkuWmM+vuSa5dtQYNM+EEEJILZJV4wxEb56POCKacsNmr73M52efnVw7agyaZ0IIIaTWOO+8wtdZMs5A9Ob5vvuiKTdMjNAVAOjbN7l21CA0z4QQQkgt8eyzwFVXma+zZpwBhm0AhQvXrFqVXDtqEJpnQgghpFbo6ACGDTNfZ9E4A6UnDJ55ZrAyd9opeHtITUHzTAghhNQKVrNpve2fNUqNPP/mN8HKXLo0eHvihouiJArNMyGEEFILWA3XW29lM6exQSnzvMkm8baF1Bw0z4QQQki1YzXKkyYBO+yQWFNCIcqY5379wi8zTDjqnDg0z4QQQkg1Yx9htq5Il1WiXCSltTX8MklVEal5FpGfisgSEXlJRG4XkV5R1kdIWFC7JKtQu6SAjORy9q3bWs22wVHnVBCZeRaRbQGcDWCYUmovAPUAjo+qPkLCgtolWYXaJQVkxzj7123Y5vl4nibEO1GHbfQA0CQiPQD0BvBBxPUREhbULskq1C7JjHG24E+3YZvnOXPCKSdKOOqcGiIzz0qp9wH8AcAyAB8C+J9S6mH7fiIyVkSeEZFnPvnkk6iaQ4hnqF2SVbxol7qtATJmnIP0uWtWr9Ybay1sg6SCKMM2NgNwFIBBALYB0EdEfmjfTyk1Qyk1TCk1bMCAAVE1hxDPULskq3jRLnVb5WTMOAPB+tw+TU16Y62YZ446p4oowzYOBfCWUuoTpVQHgLsB7BdhfYSEBbVLsgq1W8v06VP4Ojsmy79uo5owmJ3PjCRIlOZ5GYARItJbRATAKACvRFgfIWFB7ZKsQu3WKjNmAGvXmq+zZQL967aWsm1w1Dl1RBnz/CSAuwA8B+DFfF0zoqqPkLCgdklWoXZrlE8+AcaNM19nzGAF0q3dPL/xRvAGTJoU/FhSk/SIsnCl1KUALo2yDkKigNolWYXarUG22MJ8njHjbOBbt3bzbL148MvFFwc/Nmo46pxKuMIgIYQQklWs5uqzz5JrR9zYVxh89NHk2kJqDppnQgghJItYjfMttwCbbppUS+KnFmKe6ywWzbhYIKmA5pkQQgjJGlbjLAKMGZNcW5LAMM+NjeGXmRas7bGnICSJQvNMCCGEZAm7kUpiVPKee3RqvLfeir9uIBrznCZ+9zvz+TvvJNcO4kikEwYJIYQQEiJpWATF2ob16+OvHyiOeQ7K3XdX3pYouPBC8/nAgcm1gzjCkWdCCCEkC6TNOAPAHnvE3wYgvJjnY46pvC1hs3Sp+fz665NrB3GF5pkQQghJO0kb57VrC9tw5pnJxghX84TBnXYyn59xRnLtIK7QPBNCCCFpJmnjfP31hUt/P/88cN118bbBTrWaZ+t3279/cu0gJWHMMyGEEJJWkjbO9vo7OoAeKbAObubZOtEuSHlJY01Pt2JFcu0gJUnBGUAIIYSQIg4/vPB10sY5LQYTcJ8wePbZ8beF1Bw0z6QqyOWAlhZg5MikW0KId6hb4spTTwEPPGC+TplxTly7biPPvXp5L+Oll8JrTxhwKe5YCEO7NM8k8+RywKhRQHu70Y9u1KfcMYQkDXVLXOnuBr7yFfN1nEZq/Xqgqalwm4NxTly7YcQ87713OG0hmSEs7dI8k8xiXD0uW6ZPhK4u/RfYeKOEm0aIK9QtKUt9vfm8oyO+eseNA2bMMF9fcAFwxRUbXlq1e9P6E/EDdRt2a3sdryWh3WqbMMhR50gJu9+leSaZxHr1aMyvqKvT/ei6dStXJds6QpyhbklZrCZq/vz4JufZwzRWry7IsGHVbmeXuW9DA4C2BLRbbeaZREYU/S5T1ZFM0tJiXj12dOi/dXXAlCkAsGpNsq0jxBnqlpTEamBHjIgvoNgpvrlP4d3slhZg87b3C4zzMmyP8yb3RyLaDWuFQSD5kV6OOkdKFP0uzTPJJCNH6j7T3ue0tibWJELKQt0SV+wGNpdLpl4X8zbhIsGy7u0Ktg3Eu9iy5Y6oWlYajjwTj0TR79I8k0zS3AzMm6dD9Hr21CGCPXsyawFJN9QtcSSJlHAdHd7rte9nYVDz1iE2ygeVmue0XLFy1Dlyouh3GfNMMktzs36MHm2mnWluTrpVhJSGuiUFJGGcjzkGuPvu8vW2twONjSWL2n34xiE2LADWyZV+2HzzcNtBUk3Y/S7NM8k8xklBSJagbkkixtle5zvvAAMHFu+3xx7Aq6+WL+/gg8NpVy3CUefYCavfZdgGySS5HDB5cnxhgYSERYF2zztP/4But13Z40iVkQbjrJSzcRbxZpyVKhnSQUgaiMIvcOSZZA57kvN58ziCR7KBW7ovDB8O3HNPcg0j8ZIW4+xlPwdOa7gJo//vFKSm2/3vf4Mfm9SIL0edYyEqv0DzTFKJdflMu9CtaWfa2/VrYzsnXpGkKafdYW3/wWPd+xdsX7l0RVzNI0ljz9sctXFSykxuW6rOSy8Ffv3rssXVoQt13XWYup82f8/fsiiMVlbGRRcl3QKSIEn4BZpnkjrKXSkaaWeM9/v312F3XOaYJE057U64SDDB4bjjXvoVgB9yBlO1M3Om/hU3iNo4n3ACcIctlZxTnR5DL3o3KQxpW4xnu4Zs2Pbtn2wFYMtktWuf/FiO9eujaYdXOOocGkn5hUjNs4hsCuAmAHsBUABOVkoxSpU44rZ8pv1K0Ug7Y7yePRtoa9P76L+b96+0LdQu8UNZ7c5XmHCx+xSTy7ouxr+wrUPwqX+o3ZTyv/8Bp5xivo7aNDnljR4xovx+Diw/4ifY8r4bsda27374D97r3AoIQbux6rapKZJiSXyU6nObmwtHo6PwC1GPPF8D4CGl1LEi0hNA74jrIxnFevXYo4fuz0X03canngIuu0ynJa2rA/70J2DsWPPqcvbsSJpE7RJPlNPu9y/aCROwtGQZHWgA0OVt+K881G4a2XRT83ncxtmpvnffdZ4saKMO3ej9UAdW28rs2aDQ0eFWYSBqQ7fW7CTGKonEF6X63GXLgBkzgLPO0p6hoUEb5wn5W35h+YXIsm2IyCYADgRwMwAopdqVUp9HVR/JNvblM42+vqMDuPdefYXY3Q10dgJnnlk4a3b0aHP1IH0bZkVF2e+pXeKHUtq9517BjmWMMwAcVPcEAFXxLym1m1Ks3nJpeT2EVhfgHqbhwTgLFLpRh9UdllzPu+4KKIXrr9fGRYdTV6bdmtKtcTsKYKaSgJTqc6dNA047Tb+vlP5rNcxh+YUoU9UNAvAJgD+LyCIRuUlEGItKHDHikurr9UMp98GZzk5g4kTTQDc365Ppt781+qVga9VboHaJZ5y0u5N6HQoefhhbWrDkukdx7c/fAfD6ayE0h9pNG1aD9KMfAYMGxVMXEDi++QXsDYEq0vCT81ZvyGwxdizw2GPAb34DhKDdZHQb98ivNf783XfjrbuKKOcX7F/rc8+F7xdERXT7SESGAVgI4KtKqSdF5BoAK5VSl9j2GwtgLAAMHDhw6DvvvBNJe0jy5HLmFeDo0cWzYo0Ypf79gXPP1XM6rPKsqzNPkro6vfCVU9oZEXlWKTUsaDupXWLFqsvWVucZ3dZ9xo7zNpq05LpH8cWzDtEv9t8f8sQTFekW8KZd6jZG4kpJd/75wO9/X1RXUZ+7X3ltChR+ictxOX5VsL2+TqWqzx0KDH0GKM417eUzNvaPe7IeJwp6Jgy/YJjoSPyCUiqSB4CtALxteX0AgH+WOmbo0KGKVCcLFijVs6dhfZVqbNTbSu0/frxSDQ1Kiej9p09X6mtfU6quTpdRX6/UpEnFxwJ4RlG7JAQWLFCqqcnUXF2dfu2qXfP6zvXxKnYt3r58ecW6VQG0S91GiP07jqueOXOUUoV97mvY2ZM2p08vLu9vXzgvnX3ujjsqdd55xZ9BObq6ov9OnPjwQ7PeW26Jt+6MkQW/ENmEQaXURyLyrojsppT6L4BRAF6Oqj6SblpaYJlcUjgr1gljCc0hQ4C5c4FjjtG3CffeG3j8cTPNTBR5naldYmDE1hkjGN3dLtp1SgnmwPcb7sacju8WblThjUBRuykhrhHnEvUYfa6n8KFddwUWLsTYfsXlbZcDGkelsM/dbLPi0fZevcpXVl8fuJ0VsfXW5vMxY5JpQ0bIgl+IOtvGWQBuzc+cXQrgxxHXR1LKyJF61mt7u37tRci5nL4d096uT4C99y5OUxfhyoLULtkQW2dMWK2rc9Cuj0k/BcZ59mwdAxs+1G6SpMA4A1qj3cqDNlU+7KFfv4LNvZsU5uUy1ufazXRasH43Q4cm146MkAW/EKl5VkotBlBRDB+pDpqbgeuuA6ZM0f30OeeUF7LTykDGFWbUy3FTuwQwO98rr9TzpHbbTYeXNjfDeeU2Bx4+/Bp87Z/nFG40fkyV0uljxo4Nrc3UboKkxDhj3jw0H3po+XIM42xhQF0rVnT3Q30W+9xx40JpS+hY+4lnnkmuHRkhC34hymwbpAbJ5YDJkwtTyRnbzzoLeOUV4OWXgbPPLt7HjnVGbVS3CwkB3HULAC++qNMlvvKK/vviizCTinqgwDgPHmwanSFDdBk33AAcf3yF/wFJnDiM85VXFtWTW6AKtSsClDPOw4froTmHstY09stun9vQ4H3fpFcZJJn2C2VHnkVkNIA/AthGKdVm2X4rgI2UUkdG2D6Scqyr+ACFy2ROmWJmJ7DHMLW1mTNp3WbUxni7kNQg9tna1uVdAVN3c+cWHuclm8a67XdB07uvF240zFRrK7C5bTXj55/X08FJNonDONvqeG/0RXh3/G8L+ty163yEaVgZPBhYtAjNqKE+N67zjRk2NlCpX2hu1gugWOOaDWL3C+VmFAJoAvAZgOMs2zYBsBbAUUFmKbo9OPM7WxiZCOrr9d/x4/VzIytBQ4P53vTphbNnAf1+Q4P5ukeP0jNqvYIQshb4fVC72cKq3YaGwhnZ48cX6vr88/V7B2F+oYD9PAzs2wcNsrxF3WaSL3/Z+bsOE5tuivvcbk86/EKP94q2nX9+GM1LWLt+Pvu4M23EkXElA1TqF3r2NPti4zF9euXtCqrdsvcdlVLrANwK4GTL5h8AWAngnyH6eJIxZs/Wd76MGCPAvG1SV6e3G++1tuorwuHDzQvxjo7Cq8vOzsiW2iakAKt2u7q0Xo3bfUBh7Nymm+qMBS04uGSZjhj9/Ny5xaN9nZ3RrzZHouW55/TDQEU/4lwnqqDP7ewSlPspvxdHAQDe7tyusGgo/OEP5W+JVw1xr+jHUecNVOoXOjv16oFWbr451n+hAK8TBm8E8JyIbKeUeg/aSM9SSnVG1zSSZnI5YOZMsz/o0UOHXYwe7Xwr3LiNMmWKvlVjZC8gJG7s2m1oAK691rxlCACzZpnanXBRwB9cowL7D/ammwKffRasTJIurJkTYjDOjT0VVN549OgBTJ1WXpu5BQpH2xZHGYCPsQIDNrwulQaMkEoJyy+sXFlY7jbbxPt/WPFknpVSz4vIcwBOEpF7oWfE/jDKhpF009KirxIB3b//+Mdm52vthI3YJOt78+bp5bUfeaTQQDc26pOJkChx0q492cWUKcCxZ22FfuuW+6/AGMreeWfgzTcL36vx0aeqwmpsV60Kt+y5c4Fjjy3YNHmSQtclZtXr27xd1NlXFRQoiACSL6exMYMTA7MAR503EIVfaGjQmY+Swk+2jRsBnATgJwD+o3Qic1KjGDNbjdvdQ4YUvm/kXJw3T/+13hZsbtYnQ2OjPraxERg/Hpg/n6MfJHq8aHfsOEG/9gDGWeXT14kUGud993X+AW1vB37wg8Jb/yT9WI3RlVcCffuGW7bVOH/zm4BSG3Q7Qa4om7/5Uvl10bbOvpugd5NCfb1eS2TaNOA3v3Fesrjq+fTTpFtQU0ThF/7v/xLWrdfgaAAbAVgNoA3Aj4MEWJd7cPJKtpg+3ZxsZV+yeNKkwskAX/ta8WTABQv0fmFMErSCpCevkNTjql3r0r1+HmvX6uNLTRa0Y91n1CjqNit4/X7DKLu7u/T7Do+d6950bWNofW5Hh1n24sXJa9frdxHXxD1OEiyi2vyC55FnpdQqAH/Lm+e/hW3iSToplf+2tVXfRrEuWWxgvdLs7ta3XEaNKr6inDChBkc9SCz41q5IsKV7ldKzWeyxzb/7nfNo8/e+V7zvv/7lv14SP1GmpHPIuVywzeNkt9e7dyouJ08ofa5IYT5l67LTpKapJb/gd4XBrQHMUUqtiaIxJB2Uyn9rFa4heKd1451ilcqtT09IpQTVbqBJgR98oI2Dk6lxMlVLlwI7FRobTJuW3lXRSCExGuce9Qo9R+V1O3gd0Lt36eNvuAE4/fSCTf3QipX1/XB5S0h97siR+l65lU8/BTbbLITCYyCuTBs1Futcq37Bk3kWkc0AHADgawD2ibRFJFFyOTNxuUjxlaKfRUyMWKXHH3c+YQgJkyDafW/q/fjeXwKs86QUcOutwA9t86aXLAH23LN4f68Gm6STqIzzwoVFHWd9nUJ3PmWXfbKfKzbj3LtJhdfnvv02MGhQ4bZJk/QwYBp4++2kW1Cz1LJf8DryvAhAPwAXKaVeirA9JGGs68Mbwf0i7kJ2Wzc+lzNzNltXDkrrVSTJPr6169WYWHn6aWDYMO9m2Gm/9nZ/ywiTZInKONvKfR/bYGDd+xt029nlX58d9Y04+9T1mDIkpD43Cxd9t96adAtMamzUuZb9gtdUdTtE3A6SEuy3VoIIOZcDDj5Y52YEdDlpvv1CqgNf2g1yC1cp/etgT1Bu/HJYmTkTOOWUwm1nnaUTSpPsEJNx3rhXO9Z2NKAxr9uyS8Bfey1w9tkFm3o1Kt3nTguhz3U6P9avT+cS8n/9q7/933knmnbUILXsF/zGPJMqJ4z14Y2rUYOOjmycDCTbeNJuANP89+NuxVFzflDZaHMNjEJVHTEZZyiFf+XjRsfffzg2G/dA+TJsxnnyJIX2i83XgfvcyZOBiy4q3PaNbwAPPuizoBh59VV/+w8cGE07amzUGahtv0DzXMPMmGEmJbcuEuF2a8UrxtWocSXZ0JDu2CWSLay3+EaPLo6rc9VuAOPc2FOh7W8C/O3Ewje8mubu7viXBCaVE6NxBvK6DRJG9NlnwKabYmQuhD6XF32kBPQLhdA81ygzZpiT/B9+WP+1r7IWlOZmveCJm8EhJCj2W3wzZ3oYpejq0uvB+uBfg8/HvSN+hzan5Y/thuLAA/UsFysHHAA89pivOklKuP32wtdhGMhXXwX22KNgU2NPhZZcXrtBw4jyVNTn1oJpjvoC9vDDzefGUnpVBP1CMTTPNYSRUmbkSH0FaWXu3PBOBqDyq1FCrBjaXbbM5y2+gKbk6yL4+uIri7Z7Kt+P8Vi6FBgxArjvPn9tJNHQ1qZXfDSo0ETmcs4jygIF6QCe+fv7aN5vO/8FO7TLd5/rpN3x44GpU/23p9Z5wBJqY5//kFHoF0pD81wjWFPK9Oyp5y4ZV5AAMHiwDndL+wxXUntYtdujh350dOj3Ghp0flFH7fo1zoMHA3PmFB93xhnA9dcXbqvUNK9YAQwYYL7+7W+9H0uio1cv87l9YqhPnIzzVj0/xfJ2nRe5WwnwO5+FhjEivGoVsPHG0ZRdi1jjwd96K7l2hAj9QnlonmsEa0qZ9nZg002B6dP1FeTgwcB11xUnN7deedbqCUKSx6pdADj1VPO9IUOKE/Nv/vBt2GXiiU5FuaPyq7nttlvxditOpvm00/QiFV5Yuxbo06d4+333Vc2IVWaxfrcLFlR8q99unCdPUrhnpL49PdUpHKgcYZjbag7R8BKatWhR+PV+61vm8x12CL/8BKBfKA/Nc41gTynTv79OKTNxYvGJYiybaR3t+/GPsxOLRKoLq3aN30dDi5MnF2o30KSrY48tNhVvvgnsuKP5ev16oKmp+FivxkMpZ3Oc1vRftYb1+x8ypPKOzqan3k0KU/rrvtW3cV61CujbN9T2AADuvhv4zncqKzdN2BcscmLw4HDr/PRT8/lf/hJu2QlCv1AemucawZpSxr6M5pQpxctmWk+Qri69ivCf/6wD+6v5hCDpw9Du7Nl6guCNNwKzZult1k7e96ISxmjzXXcVb7dS6Wid0/FLlxav2kaSwf79PPdc8LKWLwe22qpg0+RJ2jiXzd3sQK9GhfkvVtDn3n8/cKTDCprVMtpsZcyY+Ovs39987sW8ZwT6hfLwPmGVkcvp0bhcrvi95ma9ompra+GVY2urPlEuv9y8BWOYEuvvSlubOSOWkLApp92BA83O2br869p1Emg1Nse0YVZTIVK8z4wZ3o2H0/H33KOPp3FOByGkpDN0C5Ei4wylMGGCf+O8Br0hUJX1uSLFxtmu8WriwAOdt1vnFkTFAQdEX0cE0C8EhyPPVYQ9yN8Qth37LRkjRslpHfpzzwWeeiqmf4DULF6066Rb33Gpxmiz03aDN94Adtml9D6lcCr/qKOAe+/1djyJh5CM86hR+gKugFdfNePnfWr0K8NVZX2uU32ffAJsvnkFhWYAtzkDK1ZEU5/1c85gWkr6hcrgyHOVkMvpeKS2tuJYJCfGjNETr9xOGEBvN27RGOvVjx4dQeNJTeNVu4YeR40Crrm6K1h8s5fRZrtx9jpa5zTSbBxP45wuQjLOEyc6GGeltHH+978DXdwF7nNHjHDXX7UbZ+IL+oXKiXzkWUTqATwD4H2l1Lejrq/WMFZbmzkT6OzU2ZXq6iwjcw77WwP7DUqdEC0ttTWL1oDajQ5jZrYRT9fW5k27556bNysPF7/vxhWXd+DCSxqK37CbZjtPPQXsu2/5CtwMUkK3x6nbMlRonK19blt7YVm5BQrNTnWUY+3aDRNSA/W5VZJFI1LtPvpoeGVldClu+oXwiCNs4xwArwBwSCxJKsEQ9vr15vlbVwcceqi+qnQSrj2wf/p0c/JVqROimk+CElC7EWDtkOvqtA6NTrycdotG+TxQZJytP3a33uo80cfrSLMTyf+YUrduhGCcR40CsG4d2tC78L0FKthqgZUseFIlptlCdNo9+ODQi8wS9AvhEmnYhohsB+BwADdFWU+tYgjbOBFEdNYrtxMBKA7sV6r8LZtahNqNDnuHXFcH1NeX0e5112HCRT5Myf33F2+bOLF4tNlunL2EaJQKz0jYuFC3JQghVMO4gFsbgnH+3ybbB9eLkwb79Utcf5WQGe1mdNSZfiFcoh55ngLgfAAbRVxPTWG95W3Nf+slt6I97VdXl/stmxpnCqjd0Mnl9BLbxi1AI/VRa2uJ23xBFqs44ojC1+VCNLxMqDrkEJ17yU66fkCngLot5mc/K3wdYMS5pQVFF3DXHP4whl98mO/4+95NCvMeBHwP0K1cCWyySfH2dGkwKFMQtnbHjw+tqKxCvxANkZlnEfk2gI+VUs+KyMgS+40FMBYABg4cGFVzqgb7DNmyxsMB47bK6NG1EZvkF2o3Gqzara/XE1DKJtL3Y5ybmoB16wq3rVhh5mI9+mjg738vPq6c8bjlFv1L4/e4mKFuXXjvPeCPfzRfBwzVsIcM5RYonBMgTGPyJIV5IwP0udUXorGByLQ7fXo4DTQbYD7PwGdPvxAdUY48fxXAkSLyLQC9AGwsIn9VShXcJ1VKzQAwAwCGDRuWfjUmzOzZZsySkXNxwoRgZdVKbFIAqN2Qsc7u7u7W2wYOLKG/IKPNduNcbrS53I/fe+8B229fvN2INUkf1K0T1u8wgOGZPds5o4bviYH51SR9d9dOdfzud8D55/stKc1QuxFAvxAdkf0CKKUmKKW2U0rtAOB4AI/aTwTij1xOr9pj9P/19bx9EgXUbrgYox+PPFJ+djeAYMbZijX22Ck2tKurtIkyckHbjfMbb7gvs10Jc+cCH35YcTHUrQPW7/6jj3wfnlugipbTzi1QwMUX+9OpUv6XYb/zTveLvuoyztFrd+7cysvI4Kgz/UJ0cJGUDNHSotPLAPo8PvlkXgmS9GNMVCmbUaOjQ7vqSrCa5lLvu+F03J/+BJx+emXtsvPCC8A++5iv7THapHKs3+XEicCWW/o7vqkJzevXF2w6bbzCVL/5xYMYrSoO0UiE73436RbEDv1CtMRinpVSLQBa4qirmrGv9OM1AbkxYYCxSv6hdivHrltH4xzGaHOpsoKY5t120yvFhYnb/3nzzcAWW4RWTc3r1v45X3ppRcf/RG7Gbb1Oxtpp3nU6eZLSfW4F9QLQM2ydwoeqFF/afe21KJtikrFRZ4B+IWo48pwSvAjWmPnqVdhGQvQ//1lfgZZagpOQoJTTblndhmWcwzLNXo7zQ3e3vmfqxLp1QK9e4dVFfKWkc9Su7fjcAoWd53Vi7SXeddqzQaHrlzpSw1Ofu8cezhdqGTFqiTFvnvt7V1wRXztihn4heWieU0CpNebtJ4nXoH2nhOhGfkaeDCQs/Gq3gF/+Evjtb4NXXso09+ihw0DciMM0n302cN110ddDTHwa5yLt7ldsnJv3E8+jx7nHOnDgIT3QmZdeW5uHPpchGsF57DH394LOjLOTslFn+oV0QPOcIIbQly0zF42wCrbUSeK1TKu/YH5GEhYVa7eS0eabbgJOOUUX7jQJq9QPXBym2a2OCy8EJk8Orx5SiAfjbDUX1sV62tuLjXOPeoVOH/HNkycpLLvNzCYDlJmkRdNcOf/6V9ItiA36hXRB85wQhtDb2rRQ6+rMSf5GWlp7526s6uN2G8aeR9dYiKK+Xk8WKJtTlxAP5HJ6pVsj2X59vT/t+lop0E7QEI2oTbPb4hXAhhRlJEI8Gmejz62vB376U20Q5qw7Ekd0Fa5I2aNeobPLu057Nym0X6LLbWjQNz3q6oDrr/cRpkTj7J/PPiu/j9vdHy9Yz9sEvx/6hfRB85wQLS2FOW+N1LHd3cC55wJ7710c8N+/f+krS+vJA+hFKAYOZOA/CZfZs7V2AW0Stt4a+Pjj8tr1c/u7iLa2wnVirZx1FnDttc7HRW1USo2g0wzFg8dQDWuf290N/P73QLcqPPa90RPQ5/Xn0ZnzaJyVwuTJQPslHvrd1auBjRwWz6NOouXMM4Mf294eXjsqgH4hfdA8J8TIkcV9vnFiGFeNEyYUBvxbTyCnWDqn2bU1cxLMnw8sXw4cf3zSLak5rCmK3bTrd/niApTSk6n22MP5PSc22wz4/HPv+/vFzTQPHw48+WQ4dZDy+IhxHjlSj6oZ/azdOPeoV+ic7TN3Mzz2uwzRyB4XXGA+NxxmQtAvpI9ULpNVCzQ3u6d2td6KaW7WJ0Vzs962oePvLvYGxuzayy+vkVmyLS3mIhiHHAKccAJ/kGJg9Gj35BFF2v15e3DjbCx2IlJsnK0LoVi58kq9v/3kcNvfD0uWOC+6ApgLr9A4x4f9QrnM99vcrMMoRACFwu+wvs57mEbuicJFdkr2u056+fnP2U+Fjb1D+stfKi/zyivN5wmvKEq/kEKUUql5DB06VNUSCxYo1aOH8auuVF2dfogo1dSk37cyaZJ+z9i/oaF4n6pn1SrzA7A/br9dKaUUgGcUtRsp06crVV9fRrtu35OXh3I5/tZbnRv05pvO+3d2Vv7PlmtnSFC3PnjmmeDfg+07/O7Bn3rWpVvfXMQtt0SulzSRmHaNz/Wb37Q3qLLP/IknzOPfeSdYGSFj9wv19Waf29hIvxCUoNrlyHOCNDfrTDvjxwNHH62vILu7tdSN2yxWjNuOBl1dxftULcYIjlPM4KpV+kNjyEZsjB0LPP64u3YrGm2++mr329w/+EHxNhFgp50Kt7/wgn7PbYjcC26jzF/7Wjgj2SQYSgHDhhW+9oLLktpz5/cre+i72A4CBaUKJ2M5IgKcdFLhNuolWg47LNzy9t/ffD5wYLhlB8TqF8aP1yPR1j539uzC/WvaL8QAzXPCNDcDU6fqUElr3+qU4qi5Wa8U3NCg7yI1NlZ5KhnDvDgZmF/8Aht+yTbayH0/EhlO2p2ASejqrsA4iwA/+1nxdifjYUw7t/KrX+l99947WBvmzHHXktGOGkqPlUqs33mpXN5WRIBJk8zX3/mO5+pyCxQmjX8XjY26X3ZN4eWkm5dfpmmOg69/3Xn7L37hv6xVq8znt90WrD0RYfS5U6cCW21Vft+a8gsxwwmDKWHkSC3utrYSKY6gR/z23ruKl8/8xz/cg7sA08DU1enp8k7vk1gxtLt2XYWm2W48XnsN2GWX4v2jyKDBrBnZwPo9tbSY+bW8HgOYevOCUmiG7mdHj3bpd5l6LnmcJhMDhXHLXtl4Y/P5CScEa08MjB4NzJyprx8bGpyX3656v5AgNM8pwc9Sml5XDcoUpX7M2tt171BqdHnaNGDcuGjaRkrS3BzQOD/yCHDaad4zEcRpms8+G7jmmuDlkvCxflf77w8cdJC/YwC9FLoX49zdXbSfY7/LLBrpIIq7jt/6Vvhlhkhzs/YLXpbprjq/kAJonklylOrwxowBbrnFXOrIiZ13Bl5/PZKmEY8E/dFyG/2LwzSfcooesgmzTBItdg08/rj/YwC9YE05vGjAqWwHw00S4JFHgh9r/f7++c/K2xIh9qW4SbzQPMdIKbEHWVozk+yzj57M5YZhqmbN0g+3faz7WrGujUtCw1G7QYyCW4iGcXfBStimmaEZ2cRHLmcAwK23Aj/8oe9qnpQR6P5PrvRCPgzRSD9hTx5MCPqFdEPzHBPlxO60tGbVnAzr1wNNTe7vW01wuR+n1lb3fXbaiSM/EWDX7qMPtWPEQQGXm/Yy2hyXab76ar1GM0kvfo2zbf/1W+2AXh+9Xb4aKNTXAZe3uPS7a9YAffsWb6dprh6s2kn4e61pv5ARmG0jZHI5YPJk/deK27rzBsZqPyVnc2cNY4TRyThPn164nxPG5MCHHjLL2nzz4v1Gj9b7vfFGOO2uQdx0CxRqd+068W+c3X6I7Fk0ymW58MPAgeXLo3FONx6Ns6Fdp+/ai3Hu3aTKZ9GwG2emnks/xx6bdAvKQr+QXTjyHCKlrhYNsRvvOaWh8zphMNWUG/nt1w/49FP3yX1r1gC9ewNDhpQu65BD9AdGKqbcKIeh3cDZNMqZoDBHmhmaUR34MM6jRgXUZr7MeW63x520xH4nO9x5p/d9Exh1pl/INhx5DpHZs3WEQldX8SIn9qUwgeIrTuvSmpniscdKZ8Kw5tr89NPi959+2uyw+vTR5SxeXLzfmDHmiM+8ecDbbxfmghahQQpAS4vWa1eX1q892X7zgquCG2c71u9n3LhwRprd4qgB4NFH0z9KmOa2JYGPUI3ZsyszzoBDv3vnne66pHEmIVGzfqFKoHl2odRtbLf9b77Z7JO7u8315g0MsQP6ivOSS/Rfr3WkDsOwOKWMmjPHfO6Uj/nSS80Pa9993Y33DTeY5sfIvmE8Bg0q3HfAAMY8w792+/c351kqBdx0k+VYEeC887xX/r3vOW+3GtgXXtDlzphRuE97uz8jaejAvlCKtb6DD/ZeXpyceGJh+087LekWJU4uB6zrbVvtr4QeXrjlOUyd5u98vwffQe8m5X5uiADHHVfcBl7gZIPnn/d/TAijzvQLtQfNswPG7RQ/Ym1pATo7C7ctWuS+b6l4plRjNa+l+P73i7c1Npq9xWWXuZexaJH5g/Xtb5ev8/bb9b4ff+z9/6hSgmi3tbXwdWdnfvQ5yIWI/Vap1XgYqbz22adwn8ce0/vYs224UUoLaTU6ixcX6ti+clmKF2OIg1wOuHbkXDSt+8zcWOp7FMGXfjzUVx096hW+i7ud+1wnTT3zTDq1RNwZPDj2KukXahOaZweCiNWIUbLy5z87n0iZC/Y/9tjKlr82foDa2tzLWLvWND5GvLOInvjlxOrV5v7HHx+sXVVIUO02WuYAKojvEb0i9t23eDJgfX3hPt/8pt7ngAPKl7dypbsG3347nabZapaHDHHeZ/163e4DD4y3bSnjiYfX4vZ2c4LX5EmljbNfcguUc5+7667uIRpD/ZlzkkFCGHWmX6hNaJ4dCCLW5mZg/nxg+HBzW2en84lkj2dKbcyS8cM/d67/Y1esKCzHCcPwtLXpSYLlDLqxv1I6NhrQ97VKZfWoMSrVrkIIYS9KAU89pZ+XynjxwAPlyzKO32QT5zKUAr7whcraGxZTp5a/S2JcVBgP61VLDfOLiX02PO/dpNx169c45z9nxz5XpHiRpTRehBFnjFgzp35/xIjYmkG/UJsw24YD1pms/fubgi4n2uZmYMqUwhm0bidSapfMrCRmeOZM4OST9XOnlHKA+cPkZST7738HjjyycNu557ovm2yM4tUwhnbtk/7KHje0HU8+VaGRs480l9unFFnJmuHlfFm5Ethoo+jbklUsn+F1F7yHKTu69LlBjLOFDX2uUzlcHTB7dHTov1ttVfyel9iJkDJs1LRfqGFonl0whOp3FR/jhJg7FzjmmIwIvpIfjQEDgE8+0c8N42zHj2G2d2J/+YvO41yOd98Fttuu/H41wqxZWrezZnnQbaWm4YYbzAlvlZjmF14ojoc2WL3avNuQJKefrkeYy5Emg59mrHqZPBnDDtq2uM/ddpmvuwsfHT0OW90zrXRdVvhdZRMjaHjLLZNtB2rMLxAAEYZtiMj2IjJfRF4WkSUick5UdUVFkFimXE4Pjs6bp/+mdmbs++9XFsdsYBhnO0qZmRdK1bPHHoW3sd94o/DWt5txvuqqwuNCNM5Z164v3Vb6/SuljXOp8Awvq8I5TSS0Hp+kcbbq0c04r1xZqMeEyJR27Xq58MIi7TbvJ76Mc+8mhbfOtxlnt7kWDNFIDYF0a4w8G+b5gw/8VGg+D0kDVe0XSBFRxjx3Avi5UmpPACMAnCEie0ZYn2e8ppUJEsuU+pmxhgmIYpTWftveLUm91WQsWVJoTnbZpXT5xuNnPyt876GHwlxhMNPa9aTbm26qzDgb30MYprlU+Ulw0EHesspY9ZiesIxsaNf+uea/a6t2O7v86VOgivtcEaBXr+K6aJrThn/d2sM2tt02kobVtF8grkQWtqGU+hDAh/nnq0TkFQDbAng5qjq9UG41NStBVvExTqByMUyxElUs37nnAocdBhx+ePl6vMTDljrGyqpVwMYbO79nTCKpgKxrt6xuK9HDSy8BX/xi8FvgM2cCp5zi/F6Scade6k1L6EgJsqDdIlNsW7Bk3rz8iLPXshcojBoF1Fv7XKfv8+ijgXvuCdR+Ei2BdBs0bOOII6wVl9y1Jv0C8UQsMc8isgOAIQCedHhvLICxADDQLS1ZiDhd6ZUSud9A/dQsm3nRRfpyOWxeegnYay/9fMoU/XDivffMkQCv4SFtbcX5e4DSkwSt/PWvoZuvrGrXVbeVjjYHNc1pmwBY6QVcBnDTbty6BUztljLOBn6MM5RCM8w+96he/8Ke+33DcT+SDbz2uXv27as32s3zIYeUruAf//DclprxC8Q3kZtnEekLYC6Ac5VSK+3vK6VmAJgBAMOGDYu8h4vjSi/RmbFRjNytWmXeljaMsxPWiYFewkIefBD4hsMPnZ//ob3d++IaPqkq7VZqmi+5xD08I2i9cRuaSi7gMkYp7catW0BrtaxxXrnSOSWhE1ddVRC61dzsYrppmjOFrz53s830l2vPtlFqCXXr4kRdXWXbU/V+gQQmUvMsIg3QJ8KtSqm7o6zLK1V5pRf1rW63eM7Jk4ELLwSuvRY455zy7dhiC2D58sJtr70G7Lab97bE9GNYVdoNqo+1a82JpXZKXbSccYbOwOFEnGamBkaXnUildu3G1v6Z9+qlL1y8YD/W6Xt++mlg2DDvDSSJ41u39gmDXjjxRPN5XfkpX1XpF0goRGaeRUQA3AzgFaXU1VHV40QuV1rsVXGll1RsqFLA558Dm22mFyiZMKH8/lb8tPtHP/KWsPgHP9BLdE+bBowb5718F6pGu21txZOlvNLd7fzjsnAh8JWvOB+ThlFmL/rq7Cxe7bBKSKV2XSYHur5fCuuxxx3nPCm5yi6GaoFAurXGPK9aVX5/64I4H3204WlN+AUSOlGOPH8VwI8AvCgii/PbLlJKeVhWLDh+AvzLlZO6q82OjmRuKVtjXf3mafbzw/j556Vv2ypVerRg/PhQzDOqQLu+4katGN+1/XMeOxaYPt35GLfv+FvfAv75z2Dt8EqNji6XIF3atc9VCNo/eDmudr7jasS/bq3ZNrxku9l1V/N5frS6qv0CiZQos208AYSx1q8//Ab4OxHWCRUacY8y2yeGeR1RfO654KNIdtav97/c9ocf+tvfhaxrN5BxLpd2zs5uu+mQG7eyosSLxmp0xbhUaXfg+3qyr9m4woOCGGenY7q6PN2CJ+klkG6NzErGxMFSWEOCHn98w9Oq9AskFqqux+nfX/ejdXX+A/yNfI6zZ6cg96KXPLNR1u3G4sWFeVKt7Rw61P24QYMK8+JafxCXLy8sR6S8cb7ssuLynJZpzRCVavf+4/4SfMTZa65m4/txMs5R5c+1a8MNqxZq0DgnRS4HLFsG9Ohhy3FrnTRs1UVnp6fv54HvzCi/Omm5u1GktnDLHmMNX9t//w1Pq8YvkNipquW5jdV6jIGIKVO8XwHmcsDBB2vxGz8CQMy5F598EhgxIp66dt3VfeTQymGHAQ8/rJ+LAIMHeyu/tRXo1694+6JFwJe/7LmZAIBTTwVmzPB3TMaoRLszZgBjx4VoFp0MsJvZufZa4KyzwqvbqN+LIeJt+sSxjrrV1+tTdfRo292PdevM54ccAsyfX7bc3k0K834BbbSdJqbyuydOvPNO6fevuGLD08z7BZIoVWGejXijZcu0mI07tq2t3suYPdu8s9PRofPpDx8eUwxTEqNkpYyzUvrW1oEHAv/+t7f2Of2Y3XmnntTjhyOOAO67z98xjz6qv7AHHwS++lV/xyaINU7OuH3oV7u5XIjG2c9t9bDNC2OXM4W9zzWyfg0caDPOjzxijvp5/I4nT1KYN5Kp50gZdtoJ+N73yu9n1d0FF2TfL5BUkEnzbDUdgDnyUckVoGXyLQAdAVAukURFpOm2sp8YZ+sxVs47T+de9UOpSWhO7L478N//ur+///6p/3E1tNu/vx71MOLkpkwJkE9UBKH000mY5u5ubxkvUv591gpe+9wJF1m08/Wv6x0BT33KaeMVpk4FJjjt67evINXPppvquAkj60YZPt/3MLySy6hfIKkjc+bZHpw/ZkzhyMepp+rRDz9XgLkc8IBlTm9Dg771GDpjxnhLvRY1fs3yihXa7RnstZd/8//rX+uFNry0LWgM44IFwY6LCat26+q0Zru79evWVp/5RMO4+LLHobrlbV6wIJzhFI4uZxKvfW6BcQaAhx7Sfz187wKFu/ovAsQhpIt6IKXYc0/39yza2+alhzFmdob8Akk1mTPP9tmxQOGI3ejRzidBqVQyLS3mySQCnHJKyLdekh5lvuMO4Pjjzdd+0s2JAJtv7r++73+/9D7LlgFf+IK/cq289Rawww7Bj08Aq3aNawQRc9TDLZ9ogXaHtgONjZU1pFz2Aqf9gtDusa00R6nGU5/rsAhKboFC81dLXwgfKffjfvVtKAjwW9ub1AXxgjV/swufYdPs+AWSCTJnnkeOLBb/6NGlB3TLpZJxKrNikjbMVqzG2QnrjHbrXy889hhwwAHu7193HXD22d7Ls7NypbccnhnArrMpU/SIc//+5gxteyds1W7R8sZ+icM0c3S56ijX5zoZ5w+O/yma50wpWW5ugcJ9TnHNzz0HDBkSSttJDXHMMYWvLX3RgPrP0u0XSObInHl2Wi4zlwNmzdJinjWrWOwtLTq4v7tb/7XncgxtCc40GWY3Pv4YGDDAf4wzoGcyu6UCqvR/rwEz5abdUh21od2u7go+344OHeD36aeF4TdW7KE5Xlm92tvFTQ18v9VKqT537bpi4wwRbFOu0PPP54RAEi5Oq03mufzyFPoFkmkyZ56B4tvb9tuKs2cXCrt/fzOfene3s0cIvATnqlXAxhsHODAm7PHNW2zh7bjVq4E+fYq3J22S//xn4OSToys/Ykppt60NmDhRP4x99n3rb+jqLhMC48YLLwB77x3+KDNHl2sOJ93ajfPkScp5sp8Voz+68sri7YRUgtuAkFKwzuVL1C+QqiG15tnPcpfW2yj19dpfdXaaI3mtrTq+tLtb//WTksaVLIwyA8FXY0vKJO+4o45nzjBBtGuMdDzyiM4SOG+evh1+aJAGXHkl8ItfhGeaV6zQdyvKQQOUebxq9/xfF15Y92pUWG+fMOiEXZOdnd6yrhDihKXPmTw5xX6BVB2pNM9+l7u03kZZtgy48cbC1X5GjtTzlnylAXMiK4a5FFaDY4yaB81u8e1vA/ff7++YsD/D5cu9j6bHQFDtTpyojbORfSPQSoFbbKEFv+eewPnnF79vhG94gaPLNYdn7V5zDerXr93w8rTxCuunBdArtUMqxbJA0yWXaN0W3BFx0FhsfoFUNalc19RpvXknjOUxczl9QkyYoIP36+v1b399vXklOm+ejnvyve78gw8mt0x2pRhLFc+aZW6zLnPsNdxk3rzipbCVcjbO9qWUvS6t7MaiRc51G48UGWcguHYnTtS+VkGCTwz8+GPntE3GZ1XKOH/4of8lsGl+qgpP2l26VCcoN7jrLkz1a5ypHRIWf/rThqfWbDBOxOIXSM2QypFnY715pdyv/EqNkjgljfAdo5Q1s2yPbQb8/w9r1wJNTaX3CftzqaKZ9bmcHskwPKof7QJAW3uIn+2AAdpMl8LLd7nnnsCSJeG0iaQWT9rt6tKrulk59ljvldAwk4i4oe4M1MGWkciit8j9Aqk5UjfybF9v/qyz9AhILle4n9soSUuLjl9SSv91G/lzpJIR0qTx02a3UdympvBHjl97rfTIcVjGua3NXC81Adas0Z3zjTfqf+uII/RiEk7YtfvYvI5gYRpOGJ+rk3F+803/o8s0ztGzalWixtIwFmW16zXkx84vf0njXI10d2vtpoCz1HUlow8j8Qsku7S3A+vXV1RE6syzIfLubv246irg4ouBgw8uNNBG0H99feEoidt2V047LbuGuRx2o2rN7B6WOX7//dLmeJddgrX9+efLG3nro1cv/UiIVavMzrmzE/jnP4EZM4CDDtJ/rVg12tkluOCSnpU3wO1WuPUz2nln52PHjmUoRpicf74/7W68MfCTnyTWXKuxcNVu0P5RKX3/m6Qf4+6l10d9vdbuf/6TdMvRrQS/6/z5hte9GlU0foGkk1df9afdxsbyd9nLkKqwjTVr9K1D6+Rrw++1temUMsatFLdci55yMFayBHQaMQzPmjVA377m9jAuCMJapOSll3TatCg5/fRoyy+BETdXV6cfxmhGdzdwxhn6X7drtOLR5tNOA264oXDbyy8DX/xi+WNpkr1z4onAbbdFW8fPfgbMnBltHQ7Y+1yRYu2OHRdAp9RXOohjUCihsLs1a8zn9fXAz7qu3vA6VL9A4ufNN90He8Lihz+s7HilVGoeIkNVXZ1S9fVKHX20fliHxMaPV5VReow03Y/Ro5X66KPwy+3oCPZZvvNOMp/Ds896+JrxjEpAuyJK1dUpdeKJWsNGk+vqlJo0ydLAhQsr+wyK/+Hyj0su8ff9VjOnnRa/bhsalFq9umSzktKttc+dPr1Qu77/zxdfDO97IsUk0edecIGHZsWv3Sb5olKAehsD1e+H3bGhvUazK/YLJDyWL09GuwsXlm1aUO2mauTZ+I8Bfevwpz8tfN+4wPWTR7dqwjFmzy69pqgbTjmcnVi5EthkE//lV8qppxbHNWQQQ7dKAXfcUfiR9+hhuR1YiR6NSp5+Ghg+3Pv+1c611wLnnBN/vUuWOGc3yRD2Pveb39R3Trq6AIUAWTSIP5L6ffL6u5BiNlWfAQDuxPdw3jPHF70fyC8Qb6xbB/TuHX+9xx0HzJkTf70OpMo8W+nqAhYvLk5W7ikX6UcfAVtvnUSz48ftB6ujQ39AcYen9O2bmkkkSWGsTgXo36eTT85rNMiP1axZOp+Sl5j0m24CTjnFfx1p46ab9EVV3NxzD3D00fHXmxK6uoC5c7V+fRlnmmaTpAxpW5vu72uIzaDN8xP4Ks7DVQCAHtIFKJ9+geiTPqnFijLaf6Qq8HfgQKChQQu/sRE45hj9t75e/x05skwuUsNgVJNxNoaGrI7MiltAfJgdqZ8bJTVqnK3abWgwJ6H06gVc+uF4/z+q//d/+u+YMd4zY6TVOD/+uL/JHGEZ58sv96fdGjTOTn2u5zzjRnB0NbPJJv60GxYrVvjTbo0ZZwBowjoAwL347oZtPXvVefcL1Y7fyZ9h4Te4IqOkauR5wAB9y7ulRed6bm0FpkzRf623XIylNXv2BCZcJMBFCTY6aqIYyciwYNOKXbuLFuntU6cJ8PcABR50kPP2Bx8EvvGNoM0Mh7ffBgYNir/eYcN0yAoJDatuR44Ehh9c/lbsPXXHYKsn7kJzFlfVPu444M4746/3+eeBL30p/nqrmYED8enAfdDvCb1Y1+8nrsGUrUv7hUxn00jqrgb9giOpMs8Gy5YBl11WuN68dXZsbupi7HPSEOQvPEkVxK9VC4Z2VUcn2robwik06s7LnqUlTtgxp4err0J9W+lOdfIkla7Y0ZtvTibN3223ASecEH+9xGTAgA3GGQAmXN7b0S+kNptGUr/ZxiIapCJSZZ6NhSbWrzd/U41bLdaY0X0Sa2FM1GD8WtaxavcT1Q/98/F4gViwoLJevqsr+IIWlUIznCkM3Q5pW4gJ3ee575j/XidE3aBHHgEOOyzqWor54x8Llx0nqWfNavP55vikIDTD2n3GtlrgoEH6rlzcrF4N9OkTf701TqSXHyLyDRH5r4i8ISIXltvfWGjC+P0V0fF3Ey4KOZ4sbhi/ljmCardbSTDjbP3+nXp6P/FrYRrnGolfqyb8aHfVKqB322f4T7eLu3j44cq+16VL/Wk3LOM8bpw/3dI4J47fPhfvvwcAaENPtGLzDVN9QgvN+MUv/Gk3LONcbuEx+4PGOREiM88iUg/gTwC+CWBPACeISMm8ThttpMU/XJ6GgqBbpdQwv/WWP3H37590i4kPAmm3d5f3iVYAcOih9krjmYhEM1zV+NXuRn2BFd39nN9UqtjMrlnjz1DstFM4/9jxx/vT7bRp4dRLYiFIn9tY3wkA2LJuBXr21NdLJbNpzJ/vT7t/+EM4/9yrr/rT7jbbhFMviZQo7+0OB/CGUmopAIjIHQCOAvCy2wF9Pn8fa9clYJhfeCH61e9IlvCv3dcX+6vhkUeCt84K491JIb602+edJe4lRamrr38deOih6MonWcN3n9tj5x3w4sl34/MzNgbaAUzLP6Jm4ULgK1+JoSKSZqIM29gWwLuW1+/lt7nz0Ufh1PzUU/6u9GicSSH+tRsWHR3+tEvjTArxp93GxnBq/e53/emWxpkU4r/P/fhj7H3GQZXXPH++P+3SOBOkYMKgiIwFMDb/sk2Alyou1Mvqa97YHMCKsAoLibS1KW3tAYDd4qgkEu02hJShI33fS9raA6SvTdnV7d13h3khl7bvhe0pT3a1e/DBFReRJ43fS9ralLb2AAG1G6V5fh/A9pbX2+W3FaCUmgFgBgCIyDNKqWERtskXaWsPkL42pa09gG5ThUVQuyGTtvYA6WtTCLoFPGg3zboF0tcmtqc87HPT1x4gfW1KW3uA4NqNMmzjaQC7iMggEekJ4HgA90VYHyFhQe2SrELtkixC3ZJMEdnIs1KqU0TOBPAvAPUAZiqlSsxOISQdULskq1C7JItQtyRrRBrzrJR6AMADPg6ZEVVbApK29gDpa1Pa2gOE0CZqN3TS1h4gfW0KpT0+tZu2zwBIX5vYnvKwz01fe4D0tSlt7QECtkkUc7kSQgghhBDiCS5wTgghhBBCiEcSMc/lluEUkUYRmZN//0kR2SHh9pwkIp+IyOL84ycRt2emiHwsIo5peERzbb69L4jIlxNuz0gR+Z/l8/lVxO3ZXkTmi8jLIrJERM5x2Cf0zyhtuvXYJmqX2k2ddqnbUNpE7YLaTZt2a0a3SqlYH9CTAd4EsCOAngCeB7CnbZ/TAUzLPz8ewJyE23MSgOtj/IwOBPBlAC+5vP8tAA8CEAAjADyZcHtGAvhHjJ/P1gC+nH++EYDXHL6zUD+jtOnWR5uoXWo3VdqlbqndkLVC7aZIu7Wi2yRGnjcsw6mUagdgLMNp5SgAs/LP7wIwSiSypdS8tCdWlFKPAfi0xC5HAZitNAsBbCoiWyfYnlhRSn2olHou/3wVgFdQvBpV2J9R2nTrtU2xQu2Whtr13J5YSZtuPbYpVqhdz+2JlbRpt1Z0m4R59rIM54Z9lFKdAP4HoH+C7QGAY/LD+XeJyPYO78dJcstHu9MsIs+LyIMi8sW4Ks3fohsC4EnbW2F/RmnTrdc2AdRuOahd9rnlSKNuAWq3YB9q15E0ajfzuuWEQW/cD2AHpdSXAPwb5lUu0TwH4AtKqX0AXAfg3jgqFZG+AOYCOFcptTKOOjMItVsaajedULfloXbTCbVbmqrQbRLm2csynBv2EZEeADYB0JpUe5RSrUqptvzLmwAMjagtXvG0lGlcKKVWKqVW558/AKBBRDaPsk4RaYA+EW5VSt3tsEvYn1HadOupTdRuaajd4n3Y5zqSKt0C1K7TPtSuI6nSbrXoNgnz7GUZzvsAjMk/PxbAo0qpqBJSl22PLfblSOiYmSS5D8Do/AzREQD+p5T6MKnGiMhWRoyZiAyH1lVkpjFf180AXlFKXe2yW9ifUdp066lN1G5pqN2COtnnupMq3QLUrq1OatedVGm3anSrYprxaH1Az2x8DXrW6sX5bb8GcGT+eS8AdwJ4A8BTAHZMuD2TASyBnlk7H8DuEbfndgAfAuiAjr05BcB4AOPz7wuAP+Xb+yKAYQm350zL57MQwH4Rt2d/AArACwAW5x/fivozSptuqV1qN6vapW6pXWq3OrVbK7rlCoOEEEIIIYR4hBMGCSGEEEII8QjNMyGEEEIIIR6heSaEEEIIIcQjNM+EEEIIIYR4hOaZEEIIIYQQj9A8E0IIIYQQ4hGa5xQjIpNERInIyQ7viYi0iEibiOyVRPsIcYPaJVmEuiVZhdqNF+Z5TjH5FYyehV42ci+l1HuW934K4GoAE5RSVyTUREIcoXZJFqFuSVahduOF5jnliMiXATwJveTo1/PbdgOwCHrFnK8qpboSbCIhjlC7JItQtySrULvxwbCNlKOUeg56uc+vichYEakHMBt6OckxPBFIWqF2SRahbklWoXbjgyPPGUBEGgA8DWBHAH8FcBqAnyulrk60YYSUgdolWYS6JVmF2o0HmueMICL7QJ8QDQCeAHCQUqo72VYRUh5ql2QR6pZkFWo3ehi2kR3+B6At//wBnggkQ1C7JItQtySrULsRw5HnDCAiAuBRAPsBeBPAFwB8SSn1ZqINI6QM1C7JItQtySrUbjxw5DkbnAVgJIDLAHwPQA8AM/MnCSFphtolWYS6JVmF2o0BjjynHBHZBcBiAEsANCulukRkAoBJAM5RSl2bZPsIcYPaJVmEuiVZhdqND5rnFCMidQAeBzAUwBCl1Cv57fUAFgLYE7wdQ1IItUuyCHVLsgq1Gy8M20g3P4eOW/qVcSIAQD5X40ng7RiSXqhdkkWoW5JVqN0Y4chzShGRPaBXBVoEYH+n5Oa8HUPSCLVLsgh1S7IKtRs/NM+EEEIIIYR4hGEbhBBCCCGEeITmmRBCCCGEEI/QPBNCCCGEEOIRmmdCCCGEEEI8QvNMCCGEEEKIR2ieCSGEEEII8QjNMyGEEEIIIR6heSaEEEIIIcQjNM+EEEIIIYR4hOaZEEIIIYQQj/w/wB8EG/bqah4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 864x216 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "w = np.random.rand(1)\n",
    "\n",
    "plt.figure(figsize=(12,3))\n",
    "plt.subplot(1, 4, 1)\n",
    "plot_gradient_descent(w, 0.02)\n",
    "plt.ylabel(\"y\", rotation=0, fontsize=14)\n",
    "plt.subplot(1, 4, 2)\n",
    "plot_gradient_descent(w, 0.1)\n",
    "plt.subplot(1, 4, 3)\n",
    "plot_gradient_descent(w, 0.5)\n",
    "plt.subplot(1, 4, 4)\n",
    "plot_gradient_descent(w, 1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fba538",
   "metadata": {},
   "source": [
    "# Limitation of gradient descent - running slowly when training set is large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20ecff9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1: weight = [[1.67418156]], train loss = [[4.4230188]]\n",
      "iteration: 2: weight = [[2.18099256]], train loss = [[2.23904946]]\n",
      "iteration: 3: weight = [[2.53602921]], train loss = [[1.16728132]]\n",
      "iteration: 4: weight = [[2.78474327]], train loss = [[0.64131837]]\n",
      "iteration: 5: weight = [[2.9589751]], train loss = [[0.38320563]]\n",
      "iteration: 6: weight = [[3.08102983]], train loss = [[0.25653855]]\n",
      "iteration: 7: weight = [[3.16653292]], train loss = [[0.19437754]]\n",
      "iteration: 8: weight = [[3.22643046]], train loss = [[0.16387244]]\n",
      "iteration: 9: weight = [[3.26839052]], train loss = [[0.14890227]]\n",
      "iteration: 10: weight = [[3.29778483]], train loss = [[0.14155576]]\n",
      "iteration: 11: weight = [[3.31837644]], train loss = [[0.13795052]]\n",
      "iteration: 12: weight = [[3.3328015]], train loss = [[0.13618126]]\n",
      "iteration: 13: weight = [[3.34290669]], train loss = [[0.13531302]]\n",
      "iteration: 14: weight = [[3.34998569]], train loss = [[0.13488693]]\n",
      "iteration: 15: weight = [[3.35494475]], train loss = [[0.13467783]]\n",
      "iteration: 16: weight = [[3.35841872]], train loss = [[0.13457522]]\n",
      "iteration: 17: weight = [[3.36085234]], train loss = [[0.13452486]]\n",
      "iteration: 18: weight = [[3.36255717]], train loss = [[0.13450015]]\n",
      "iteration: 19: weight = [[3.36375145]], train loss = [[0.13448802]]\n",
      "iteration: 20: weight = [[3.36458809]], train loss = [[0.13448207]]\n",
      "iteration: 21: weight = [[3.36517417]], train loss = [[0.13447915]]\n",
      "iteration: 22: weight = [[3.36558475]], train loss = [[0.13447771]]\n",
      "iteration: 23: weight = [[3.36587236]], train loss = [[0.13447701]]\n",
      "iteration: 24: weight = [[3.36607385]], train loss = [[0.13447666]]\n",
      "iteration: 25: weight = [[3.366215]], train loss = [[0.1344765]]\n",
      "iteration: 26: weight = [[3.36631387]], train loss = [[0.13447641]]\n",
      "iteration: 27: weight = [[3.36638314]], train loss = [[0.13447637]]\n",
      "iteration: 28: weight = [[3.36643166]], train loss = [[0.13447635]]\n",
      "iteration: 29: weight = [[3.36646566]], train loss = [[0.13447634]]\n",
      "iteration: 30: weight = [[3.36648947]], train loss = [[0.13447634]]\n",
      "iteration: 31: weight = [[3.36650615]], train loss = [[0.13447633]]\n",
      "iteration: 32: weight = [[3.36651784]], train loss = [[0.13447633]]\n",
      "iteration: 33: weight = [[3.36652602]], train loss = [[0.13447633]]\n",
      "iteration: 34: weight = [[3.36653176]], train loss = [[0.13447633]]\n",
      "iteration: 35: weight = [[3.36653577]], train loss = [[0.13447633]]\n",
      "iteration: 36: weight = [[3.36653859]], train loss = [[0.13447633]]\n",
      "iteration: 37: weight = [[3.36654056]], train loss = [[0.13447633]]\n",
      "iteration: 38: weight = [[3.36654194]], train loss = [[0.13447633]]\n",
      "iteration: 39: weight = [[3.36654291]], train loss = [[0.13447633]]\n",
      "iteration: 40: weight = [[3.36654359]], train loss = [[0.13447633]]\n",
      "iteration: 41: weight = [[3.36654406]], train loss = [[0.13447633]]\n",
      "iteration: 42: weight = [[3.36654439]], train loss = [[0.13447633]]\n",
      "iteration: 43: weight = [[3.36654463]], train loss = [[0.13447633]]\n",
      "iteration: 44: weight = [[3.36654479]], train loss = [[0.13447633]]\n",
      "iteration: 45: weight = [[3.3665449]], train loss = [[0.13447633]]\n",
      "iteration: 46: weight = [[3.36654498]], train loss = [[0.13447633]]\n",
      "iteration: 47: weight = [[3.36654504]], train loss = [[0.13447633]]\n",
      "iteration: 48: weight = [[3.36654508]], train loss = [[0.13447633]]\n",
      "iteration: 49: weight = [[3.36654511]], train loss = [[0.13447633]]\n",
      "iteration: 50: weight = [[3.36654513]], train loss = [[0.13447633]]\n",
      "iteration: 51: weight = [[3.36654514]], train loss = [[0.13447633]]\n",
      "iteration: 52: weight = [[3.36654515]], train loss = [[0.13447633]]\n",
      "iteration: 53: weight = [[3.36654516]], train loss = [[0.13447633]]\n",
      "iteration: 54: weight = [[3.36654516]], train loss = [[0.13447633]]\n",
      "iteration: 55: weight = [[3.36654516]], train loss = [[0.13447633]]\n",
      "iteration: 56: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 57: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 58: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 59: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 60: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 61: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 62: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 63: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 64: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 65: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 66: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 67: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 68: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 69: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 70: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 71: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 72: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 73: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 74: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 75: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 76: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 77: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 78: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 79: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 80: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 81: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 82: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 83: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 84: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 85: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 86: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 87: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 88: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 89: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 90: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 91: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 92: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 93: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 94: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 95: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 96: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 97: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 98: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 99: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 100: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 101: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 102: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 103: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 104: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 105: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 106: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 107: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 108: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 109: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 110: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 111: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 112: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 113: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 114: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 115: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 116: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 117: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 118: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 119: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 120: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 121: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 122: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 123: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 124: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 125: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 126: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 127: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 128: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 129: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 130: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 131: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 132: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 133: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 134: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 135: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 136: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 137: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 138: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 139: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 140: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 141: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 142: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 143: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 144: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 145: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 146: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 147: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 148: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 149: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 150: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 151: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 152: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 153: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 154: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 155: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 156: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 157: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 158: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 159: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 160: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 161: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 162: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 163: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 164: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 165: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 166: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 167: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 168: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 169: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 170: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 171: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 172: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 173: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 174: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 175: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 176: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 177: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 178: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 179: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 180: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 181: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 182: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 183: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 184: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 185: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 186: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 187: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 188: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 189: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 190: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 191: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 192: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 193: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 194: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 195: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 196: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 197: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 198: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 199: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 200: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 201: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 202: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 203: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 204: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 205: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 206: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 207: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 208: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 209: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 210: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 211: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 212: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 213: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 214: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 215: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 216: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 217: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 218: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 219: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 220: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 221: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 222: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 223: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 224: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 225: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 226: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 227: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 228: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 229: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 230: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 231: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 232: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 233: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 234: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 235: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 236: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 237: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 238: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 239: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 240: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 241: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 242: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 243: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 244: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 245: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 246: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 247: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 248: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 249: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 250: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 251: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 252: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 253: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 254: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 255: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 256: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 257: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 258: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 259: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 260: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 261: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 262: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 263: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 264: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 265: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 266: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 267: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 268: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 269: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 270: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 271: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 272: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 273: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 274: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 275: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 276: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 277: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 278: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 279: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 280: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 281: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 282: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 283: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 284: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 285: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 286: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 287: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 288: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 289: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 290: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 291: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 292: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 293: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 294: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 295: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 296: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 297: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 298: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 299: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 300: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 301: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 302: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 303: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 304: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 305: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 306: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 307: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 308: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 309: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 310: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 311: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 312: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 313: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 314: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 315: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 316: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 317: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 318: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 319: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 320: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 321: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 322: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 323: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 324: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 325: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 326: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 327: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 328: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 329: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 330: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 331: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 332: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 333: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 334: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 335: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 336: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 337: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 338: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 339: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 340: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 341: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 342: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 343: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 344: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 345: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 346: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 347: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 348: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 349: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 350: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 351: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 352: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 353: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 354: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 355: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 356: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 357: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 358: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 359: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 360: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 361: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 362: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 363: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 364: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 365: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 366: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 367: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 368: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 369: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 370: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 371: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 372: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 373: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 374: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 375: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 376: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 377: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 378: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 379: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 380: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 381: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 382: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 383: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 384: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 385: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 386: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 387: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 388: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 389: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 390: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 391: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 392: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 393: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 394: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 395: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 396: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 397: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 398: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 399: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 400: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 401: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 402: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 403: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 404: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 405: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 406: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 407: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 408: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 409: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 410: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 411: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 412: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 413: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 414: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 415: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 416: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 417: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 418: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 419: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 420: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 421: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 422: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 423: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 424: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 425: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 426: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 427: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 428: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 429: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 430: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 431: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 432: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 433: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 434: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 435: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 436: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 437: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 438: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 439: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 440: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 441: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 442: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 443: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 444: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 445: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 446: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 447: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 448: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 449: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 450: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 451: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 452: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 453: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 454: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 455: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 456: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 457: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 458: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 459: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 460: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 461: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 462: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 463: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 464: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 465: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 466: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 467: weight = [[3.36654517]], train loss = [[0.13447633]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 468: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 469: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 470: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 471: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 472: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 473: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 474: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 475: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 476: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 477: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 478: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 479: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 480: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 481: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 482: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 483: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 484: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 485: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 486: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 487: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 488: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 489: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 490: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 491: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 492: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 493: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 494: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 495: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 496: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 497: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 498: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 499: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 500: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 501: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 502: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 503: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 504: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 505: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 506: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 507: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 508: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 509: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 510: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 511: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 512: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 513: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 514: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 515: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 516: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 517: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 518: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 519: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 520: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 521: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 522: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 523: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 524: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 525: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 526: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 527: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 528: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 529: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 530: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 531: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 532: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 533: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 534: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 535: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 536: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 537: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 538: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 539: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 540: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 541: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 542: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 543: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 544: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 545: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 546: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 547: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 548: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 549: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 550: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 551: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 552: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 553: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 554: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 555: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 556: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 557: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 558: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 559: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 560: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 561: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 562: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 563: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 564: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 565: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 566: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 567: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 568: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 569: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 570: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 571: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 572: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 573: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 574: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 575: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 576: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 577: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 578: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 579: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 580: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 581: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 582: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 583: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 584: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 585: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 586: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 587: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 588: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 589: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 590: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 591: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 592: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 593: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 594: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 595: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 596: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 597: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 598: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 599: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 600: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 601: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 602: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 603: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 604: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 605: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 606: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 607: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 608: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 609: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 610: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 611: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 612: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 613: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 614: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 615: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 616: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 617: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 618: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 619: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 620: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 621: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 622: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 623: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 624: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 625: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 626: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 627: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 628: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 629: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 630: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 631: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 632: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 633: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 634: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 635: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 636: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 637: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 638: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 639: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 640: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 641: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 642: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 643: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 644: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 645: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 646: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 647: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 648: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 649: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 650: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 651: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 652: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 653: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 654: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 655: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 656: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 657: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 658: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 659: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 660: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 661: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 662: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 663: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 664: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 665: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 666: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 667: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 668: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 669: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 670: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 671: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 672: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 673: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 674: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 675: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 676: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 677: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 678: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 679: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 680: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 681: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 682: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 683: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 684: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 685: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 686: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 687: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 688: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 689: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 690: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 691: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 692: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 693: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 694: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 695: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 696: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 697: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 698: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 699: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 700: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 701: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 702: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 703: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 704: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 705: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 706: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 707: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 708: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 709: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 710: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 711: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 712: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 713: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 714: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 715: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 716: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 717: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 718: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 719: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 720: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 721: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 722: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 723: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 724: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 725: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 726: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 727: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 728: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 729: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 730: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 731: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 732: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 733: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 734: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 735: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 736: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 737: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 738: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 739: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 740: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 741: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 742: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 743: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 744: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 745: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 746: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 747: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 748: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 749: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 750: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 751: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 752: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 753: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 754: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 755: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 756: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 757: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 758: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 759: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 760: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 761: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 762: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 763: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 764: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 765: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 766: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 767: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 768: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 769: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 770: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 771: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 772: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 773: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 774: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 775: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 776: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 777: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 778: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 779: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 780: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 781: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 782: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 783: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 784: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 785: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 786: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 787: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 788: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 789: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 790: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 791: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 792: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 793: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 794: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 795: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 796: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 797: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 798: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 799: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 800: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 801: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 802: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 803: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 804: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 805: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 806: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 807: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 808: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 809: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 810: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 811: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 812: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 813: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 814: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 815: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 816: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 817: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 818: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 819: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 820: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 821: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 822: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 823: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 824: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 825: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 826: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 827: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 828: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 829: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 830: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 831: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 832: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 833: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 834: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 835: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 836: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 837: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 838: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 839: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 840: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 841: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 842: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 843: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 844: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 845: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 846: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 847: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 848: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 849: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 850: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 851: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 852: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 853: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 854: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 855: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 856: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 857: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 858: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 859: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 860: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 861: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 862: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 863: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 864: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 865: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 866: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 867: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 868: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 869: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 870: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 871: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 872: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 873: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 874: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 875: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 876: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 877: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 878: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 879: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 880: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 881: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 882: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 883: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 884: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 885: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 886: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 887: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 888: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 889: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 890: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 891: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 892: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 893: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 894: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 895: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 896: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 897: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 898: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 899: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 900: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 901: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 902: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 903: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 904: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 905: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 906: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 907: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 908: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 909: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 910: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 911: weight = [[3.36654517]], train loss = [[0.13447633]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 912: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 913: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 914: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 915: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 916: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 917: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 918: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 919: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 920: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 921: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 922: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 923: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 924: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 925: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 926: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 927: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 928: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 929: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 930: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 931: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 932: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 933: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 934: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 935: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 936: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 937: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 938: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 939: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 940: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 941: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 942: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 943: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 944: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 945: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 946: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 947: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 948: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 949: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 950: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 951: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 952: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 953: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 954: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 955: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 956: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 957: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 958: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 959: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 960: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 961: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 962: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 963: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 964: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 965: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 966: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 967: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 968: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 969: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 970: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 971: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 972: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 973: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 974: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 975: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 976: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 977: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 978: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 979: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 980: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 981: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 982: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 983: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 984: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 985: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 986: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 987: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 988: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 989: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 990: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 991: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 992: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 993: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 994: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 995: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 996: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 997: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 998: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 999: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "iteration: 1000: weight = [[3.36654517]], train loss = [[0.13447633]]\n",
      "the time used for optimization: 0.4728729724884033\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "eta = 0.1 #Learning rate\n",
    "n_iter = 1000 #Number of iterations for weight updates\n",
    "d_train = 100 #Number of training samples\n",
    "\n",
    "w = np.random.rand(1) #Weight vector\n",
    "\n",
    "t1 = time.time()\n",
    "for i in range(n_iter):\n",
    "    gradient = 2/d_train * (w.dot(X.T) - y.T).dot(X)\n",
    "    w = w - eta * gradient\n",
    "    train_loss = 1/d_train * (w.dot(X.T) - y.T).dot((w.dot(X.T) - y.T).T)\n",
    "    print('iteration: {}: weight = {}, train loss = {}'.format(i + 1, w, train_loss))\n",
    "t2 = time.time()\n",
    "print('the time used for optimization: {}'.format(t2 - t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2ad75fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1: weight = [[1.13996483]], train loss = [[6.71303793]]\n",
      "iteration: 2: weight = [[1.72585949]], train loss = [[3.72908927]]\n",
      "iteration: 3: weight = [[2.15856045]], train loss = [[2.10156306]]\n",
      "iteration: 4: weight = [[2.47812323]], train loss = [[1.21386629]]\n",
      "iteration: 5: weight = [[2.71413002]], train loss = [[0.72969249]]\n",
      "iteration: 6: weight = [[2.88842819]], train loss = [[0.46561102]]\n",
      "iteration: 7: weight = [[3.01715267]], train loss = [[0.32157386]]\n",
      "iteration: 8: weight = [[3.1122196]], train loss = [[0.24301209]]\n",
      "iteration: 9: weight = [[3.18242941]], train loss = [[0.20016238]]\n",
      "iteration: 10: weight = [[3.23428149]], train loss = [[0.176791]]\n",
      "iteration: 11: weight = [[3.27257581]], train loss = [[0.16404361]]\n",
      "iteration: 12: weight = [[3.30085733]], train loss = [[0.15709084]]\n",
      "iteration: 13: weight = [[3.32174409]], train loss = [[0.15329861]]\n",
      "iteration: 14: weight = [[3.3371696]], train loss = [[0.15123023]]\n",
      "iteration: 15: weight = [[3.3485618]], train loss = [[0.15010208]]\n",
      "iteration: 16: weight = [[3.35697529]], train loss = [[0.14948675]]\n",
      "iteration: 17: weight = [[3.36318891]], train loss = [[0.14915114]]\n",
      "iteration: 18: weight = [[3.36777785]], train loss = [[0.14896808]]\n",
      "iteration: 19: weight = [[3.37116692]], train loss = [[0.14886824]]\n",
      "iteration: 20: weight = [[3.37366986]], train loss = [[0.14881379]]\n",
      "iteration: 21: weight = [[3.37551835]], train loss = [[0.14878408]]\n",
      "iteration: 22: weight = [[3.37688352]], train loss = [[0.14876788]]\n",
      "iteration: 23: weight = [[3.37789174]], train loss = [[0.14875905]]\n",
      "iteration: 24: weight = [[3.37863633]], train loss = [[0.14875423]]\n",
      "iteration: 25: weight = [[3.37918624]], train loss = [[0.1487516]]\n",
      "iteration: 26: weight = [[3.37959237]], train loss = [[0.14875016]]\n",
      "iteration: 27: weight = [[3.3798923]], train loss = [[0.14874938]]\n",
      "iteration: 28: weight = [[3.38011382]], train loss = [[0.14874896]]\n",
      "iteration: 29: weight = [[3.38027741]], train loss = [[0.14874872]]\n",
      "iteration: 30: weight = [[3.38039823]], train loss = [[0.1487486]]\n",
      "iteration: 31: weight = [[3.38048745]], train loss = [[0.14874853]]\n",
      "iteration: 32: weight = [[3.38055335]], train loss = [[0.14874849]]\n",
      "iteration: 33: weight = [[3.38060202]], train loss = [[0.14874847]]\n",
      "iteration: 34: weight = [[3.38063796]], train loss = [[0.14874846]]\n",
      "iteration: 35: weight = [[3.38066451]], train loss = [[0.14874845]]\n",
      "iteration: 36: weight = [[3.38068411]], train loss = [[0.14874845]]\n",
      "iteration: 37: weight = [[3.38069859]], train loss = [[0.14874845]]\n",
      "iteration: 38: weight = [[3.38070928]], train loss = [[0.14874845]]\n",
      "iteration: 39: weight = [[3.38071718]], train loss = [[0.14874845]]\n",
      "iteration: 40: weight = [[3.38072301]], train loss = [[0.14874844]]\n",
      "iteration: 41: weight = [[3.38072732]], train loss = [[0.14874844]]\n",
      "iteration: 42: weight = [[3.3807305]], train loss = [[0.14874844]]\n",
      "iteration: 43: weight = [[3.38073285]], train loss = [[0.14874844]]\n",
      "iteration: 44: weight = [[3.38073458]], train loss = [[0.14874844]]\n",
      "iteration: 45: weight = [[3.38073586]], train loss = [[0.14874844]]\n",
      "iteration: 46: weight = [[3.38073681]], train loss = [[0.14874844]]\n",
      "iteration: 47: weight = [[3.38073751]], train loss = [[0.14874844]]\n",
      "iteration: 48: weight = [[3.38073802]], train loss = [[0.14874844]]\n",
      "iteration: 49: weight = [[3.38073841]], train loss = [[0.14874844]]\n",
      "iteration: 50: weight = [[3.38073869]], train loss = [[0.14874844]]\n",
      "iteration: 51: weight = [[3.38073889]], train loss = [[0.14874844]]\n",
      "iteration: 52: weight = [[3.38073905]], train loss = [[0.14874844]]\n",
      "iteration: 53: weight = [[3.38073916]], train loss = [[0.14874844]]\n",
      "iteration: 54: weight = [[3.38073925]], train loss = [[0.14874844]]\n",
      "iteration: 55: weight = [[3.38073931]], train loss = [[0.14874844]]\n",
      "iteration: 56: weight = [[3.38073935]], train loss = [[0.14874844]]\n",
      "iteration: 57: weight = [[3.38073939]], train loss = [[0.14874844]]\n",
      "iteration: 58: weight = [[3.38073941]], train loss = [[0.14874844]]\n",
      "iteration: 59: weight = [[3.38073943]], train loss = [[0.14874844]]\n",
      "iteration: 60: weight = [[3.38073944]], train loss = [[0.14874844]]\n",
      "iteration: 61: weight = [[3.38073945]], train loss = [[0.14874844]]\n",
      "iteration: 62: weight = [[3.38073946]], train loss = [[0.14874844]]\n",
      "iteration: 63: weight = [[3.38073947]], train loss = [[0.14874844]]\n",
      "iteration: 64: weight = [[3.38073947]], train loss = [[0.14874844]]\n",
      "iteration: 65: weight = [[3.38073947]], train loss = [[0.14874844]]\n",
      "iteration: 66: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 67: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 68: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 69: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 70: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 71: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 72: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 73: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 74: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 75: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 76: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 77: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 78: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 79: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 80: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 81: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 82: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 83: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 84: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 85: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 86: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 87: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 88: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 89: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 90: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 91: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 92: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 93: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 94: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 95: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 96: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 97: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 98: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 99: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 100: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 101: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 102: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 103: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 104: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 105: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 106: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 107: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 108: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 109: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 110: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 111: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 112: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 113: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 114: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 115: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 116: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 117: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 118: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 119: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 120: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 121: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 122: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 123: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 124: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 125: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 126: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 127: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 128: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 129: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 130: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 131: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 132: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 133: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 134: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 135: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 136: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 137: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 138: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 139: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 140: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 141: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 142: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 143: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 144: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 145: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 146: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 147: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 148: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 149: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 150: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 151: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 152: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 153: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 154: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 155: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 156: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 157: weight = [[3.38073948]], train loss = [[0.14874844]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 158: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 159: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 160: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 161: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 162: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 163: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 164: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 165: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 166: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 167: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 168: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 169: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 170: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 171: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 172: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 173: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 174: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 175: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 176: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 177: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 178: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 179: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 180: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 181: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 182: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 183: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 184: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 185: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 186: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 187: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 188: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 189: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 190: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 191: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 192: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 193: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 194: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 195: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 196: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 197: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 198: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 199: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 200: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 201: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 202: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 203: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 204: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 205: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 206: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 207: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 208: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 209: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 210: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 211: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 212: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 213: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 214: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 215: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 216: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 217: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 218: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 219: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 220: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 221: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 222: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 223: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 224: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 225: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 226: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 227: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 228: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 229: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 230: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 231: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 232: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 233: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 234: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 235: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 236: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 237: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 238: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 239: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 240: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 241: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 242: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 243: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 244: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 245: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 246: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 247: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 248: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 249: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 250: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 251: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 252: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 253: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 254: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 255: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 256: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 257: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 258: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 259: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 260: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 261: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 262: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 263: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 264: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 265: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 266: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 267: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 268: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 269: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 270: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 271: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 272: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 273: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 274: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 275: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 276: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 277: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 278: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 279: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 280: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 281: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 282: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 283: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 284: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 285: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 286: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 287: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 288: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 289: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 290: weight = [[3.38073948]], train loss = [[0.14874844]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 291: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 292: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 293: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 294: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 295: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 296: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 297: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 298: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 299: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 300: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 301: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 302: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 303: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 304: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 305: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 306: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 307: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 308: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 309: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 310: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 311: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 312: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 313: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 314: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 315: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 316: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 317: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 318: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 319: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 320: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 321: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 322: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 323: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 324: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 325: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 326: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 327: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 328: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 329: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 330: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 331: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 332: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 333: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 334: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 335: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 336: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 337: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 338: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 339: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 340: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 341: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 342: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 343: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 344: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 345: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 346: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 347: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 348: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 349: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 350: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 351: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 352: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 353: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 354: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 355: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 356: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 357: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 358: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 359: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 360: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 361: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 362: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 363: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 364: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 365: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 366: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 367: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 368: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 369: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 370: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 371: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 372: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 373: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 374: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 375: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 376: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 377: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 378: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 379: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 380: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 381: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 382: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 383: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 384: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 385: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 386: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 387: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 388: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 389: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 390: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 391: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 392: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 393: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 394: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 395: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 396: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 397: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 398: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 399: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 400: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 401: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 402: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 403: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 404: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 405: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 406: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 407: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 408: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 409: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 410: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 411: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 412: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 413: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 414: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 415: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 416: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 417: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 418: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 419: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 420: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 421: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 422: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 423: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 424: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 425: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 426: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 427: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 428: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 429: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 430: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 431: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 432: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 433: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 434: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 435: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 436: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 437: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 438: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 439: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 440: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 441: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 442: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 443: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 444: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 445: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 446: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 447: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 448: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 449: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 450: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 451: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 452: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 453: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 454: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 455: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 456: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 457: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 458: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 459: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 460: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 461: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 462: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 463: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 464: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 465: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 466: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 467: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 468: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 469: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 470: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 471: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 472: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 473: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 474: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 475: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 476: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 477: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 478: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 479: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 480: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 481: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 482: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 483: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 484: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 485: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 486: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 487: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 488: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 489: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 490: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 491: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 492: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 493: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 494: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 495: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 496: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 497: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 498: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 499: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 500: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 501: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 502: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 503: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 504: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 505: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 506: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 507: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 508: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 509: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 510: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 511: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 512: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 513: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 514: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 515: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 516: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 517: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 518: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 519: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 520: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 521: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 522: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 523: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 524: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 525: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 526: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 527: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 528: weight = [[3.38073948]], train loss = [[0.14874844]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 529: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 530: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 531: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 532: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 533: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 534: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 535: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 536: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 537: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 538: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 539: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 540: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 541: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 542: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 543: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 544: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 545: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 546: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 547: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 548: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 549: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 550: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 551: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 552: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 553: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 554: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 555: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 556: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 557: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 558: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 559: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 560: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 561: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 562: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 563: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 564: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 565: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 566: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 567: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 568: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 569: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 570: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 571: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 572: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 573: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 574: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 575: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 576: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 577: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 578: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 579: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 580: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 581: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 582: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 583: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 584: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 585: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 586: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 587: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 588: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 589: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 590: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 591: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 592: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 593: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 594: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 595: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 596: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 597: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 598: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 599: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 600: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 601: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 602: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 603: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 604: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 605: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 606: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 607: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 608: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 609: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 610: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 611: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 612: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 613: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 614: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 615: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 616: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 617: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 618: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 619: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 620: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 621: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 622: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 623: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 624: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 625: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 626: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 627: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 628: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 629: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 630: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 631: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 632: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 633: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 634: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 635: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 636: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 637: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 638: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 639: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 640: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 641: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 642: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 643: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 644: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 645: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 646: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 647: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 648: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 649: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 650: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 651: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 652: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 653: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 654: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 655: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 656: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 657: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 658: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 659: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 660: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 661: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 662: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 663: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 664: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 665: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 666: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 667: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 668: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 669: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 670: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 671: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 672: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 673: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 674: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 675: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 676: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 677: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 678: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 679: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 680: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 681: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 682: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 683: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 684: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 685: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 686: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 687: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 688: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 689: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 690: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 691: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 692: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 693: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 694: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 695: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 696: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 697: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 698: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 699: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 700: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 701: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 702: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 703: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 704: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 705: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 706: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 707: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 708: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 709: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 710: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 711: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 712: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 713: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 714: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 715: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 716: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 717: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 718: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 719: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 720: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 721: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 722: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 723: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 724: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 725: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 726: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 727: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 728: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 729: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 730: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 731: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 732: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 733: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 734: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 735: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 736: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 737: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 738: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 739: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 740: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 741: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 742: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 743: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 744: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 745: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 746: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 747: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 748: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 749: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 750: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 751: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 752: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 753: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 754: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 755: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 756: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 757: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 758: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 759: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 760: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 761: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 762: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 763: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 764: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 765: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 766: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 767: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 768: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 769: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 770: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 771: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 772: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 773: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 774: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 775: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 776: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 777: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 778: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 779: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 780: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 781: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 782: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 783: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 784: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 785: weight = [[3.38073948]], train loss = [[0.14874844]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 786: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 787: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 788: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 789: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 790: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 791: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 792: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 793: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 794: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 795: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 796: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 797: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 798: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 799: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 800: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 801: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 802: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 803: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 804: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 805: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 806: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 807: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 808: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 809: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 810: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 811: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 812: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 813: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 814: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 815: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 816: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 817: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 818: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 819: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 820: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 821: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 822: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 823: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 824: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 825: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 826: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 827: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 828: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 829: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 830: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 831: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 832: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 833: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 834: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 835: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 836: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 837: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 838: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 839: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 840: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 841: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 842: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 843: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 844: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 845: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 846: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 847: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 848: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 849: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 850: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 851: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 852: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 853: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 854: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 855: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 856: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 857: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 858: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 859: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 860: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 861: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 862: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 863: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 864: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 865: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 866: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 867: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 868: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 869: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 870: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 871: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 872: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 873: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 874: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 875: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 876: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 877: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 878: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 879: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 880: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 881: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 882: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 883: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 884: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 885: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 886: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 887: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 888: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 889: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 890: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 891: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 892: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 893: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 894: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 895: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 896: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 897: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 898: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 899: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 900: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 901: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 902: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 903: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 904: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 905: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 906: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 907: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 908: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 909: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 910: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 911: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 912: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 913: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 914: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 915: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 916: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 917: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 918: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 919: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 920: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 921: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 922: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 923: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 924: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 925: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 926: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 927: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 928: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 929: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 930: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 931: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 932: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 933: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 934: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 935: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 936: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 937: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 938: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 939: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 940: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 941: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 942: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 943: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 944: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 945: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 946: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 947: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 948: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 949: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 950: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 951: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 952: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 953: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 954: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 955: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 956: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 957: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 958: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 959: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 960: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 961: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 962: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 963: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 964: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 965: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 966: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 967: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 968: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 969: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 970: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 971: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 972: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 973: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 974: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 975: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 976: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 977: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 978: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 979: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 980: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 981: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 982: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 983: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 984: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 985: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 986: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 987: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 988: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 989: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 990: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 991: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 992: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 993: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 994: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 995: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 996: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 997: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 998: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 999: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "iteration: 1000: weight = [[3.38073948]], train loss = [[0.14874844]]\n",
      "the time used for optimization: 0.8996751308441162\n"
     ]
    }
   ],
   "source": [
    "X_new = 2 * np.random.rand(10000, 1)\n",
    "y_new = 3 * X_new + np.random.rand(10000, 1)\n",
    "\n",
    "eta = 0.1 #Learning rate\n",
    "n_iter = 1000 #Number of iterations for weight updates\n",
    "d_train = 10000 #Number of training samples\n",
    "\n",
    "w = np.random.rand(1) #Weight vector\n",
    "\n",
    "t1 = time.time()\n",
    "for i in range(n_iter):\n",
    "    gradient = 2/d_train * (w.dot(X_new.T) - y_new.T).dot(X_new)\n",
    "    w = w - eta * gradient\n",
    "    train_loss = 1/d_train * (w.dot(X_new.T) - y_new.T).dot((w.dot(X_new.T) - y_new.T).T)\n",
    "    print('iteration: {}: weight = {}, train loss = {}'.format(i + 1, w, train_loss))\n",
    "t2 = time.time()\n",
    "print('the time used for optimization: {}'.format(t2 - t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a792d997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1: weight = [[1.22104441 0.99143816]], train loss = [[2.09212325]]\n",
      "iteration: 2: weight = [[1.4626654  1.31550728]], train loss = [[0.83116851]]\n",
      "iteration: 3: weight = [[1.57554527 1.50532883]], train loss = [[0.45091033]]\n",
      "iteration: 4: weight = [[1.62062937 1.62222   ]], train loss = [[0.32530202]]\n",
      "iteration: 5: weight = [[1.63045687 1.69906478]], train loss = [[0.2747649]]\n",
      "iteration: 6: weight = [[1.62237152 1.75352759]], train loss = [[0.24760129]]\n",
      "iteration: 7: weight = [[1.60559048 1.79512425]], train loss = [[0.22876625]]\n",
      "iteration: 8: weight = [[1.5849866 1.829006 ]], train loss = [[0.21374104]]\n",
      "iteration: 9: weight = [[1.56311201 1.85798464]], train loss = [[0.20104812]]\n",
      "iteration: 10: weight = [[1.5412807  1.88361644]], train loss = [[0.19010441]]\n",
      "iteration: 11: weight = [[1.52014759 1.90678234]], train loss = [[0.1806036]]\n",
      "iteration: 12: weight = [[1.50001813 1.92799876]], train loss = [[0.17233655]]\n",
      "iteration: 13: weight = [[1.48101376 1.94758402]], train loss = [[0.16513764]]\n",
      "iteration: 14: weight = [[1.46316026 1.96574765]], train loss = [[0.15886729]]\n",
      "iteration: 15: weight = [[1.44643484 1.98263837]], train loss = [[0.1534053]]\n",
      "iteration: 16: weight = [[1.43079117 1.99836989]], train loss = [[0.14864731]]\n",
      "iteration: 17: weight = [[1.41617258 2.01303492]], train loss = [[0.14450256]]\n",
      "iteration: 18: weight = [[1.40251898 2.02671283]], train loss = [[0.140892]]\n",
      "iteration: 19: weight = [[1.38977049 2.03947385]], train loss = [[0.13774677]]\n",
      "iteration: 20: weight = [[1.37786911 2.05138146]], train loss = [[0.13500691]]\n",
      "iteration: 21: weight = [[1.36675966 2.06249382]], train loss = [[0.13262016]]\n",
      "iteration: 22: weight = [[1.35639002 2.07286462]], train loss = [[0.13054102]]\n",
      "iteration: 23: weight = [[1.34671122 2.08254365]], train loss = [[0.12872984]]\n",
      "iteration: 24: weight = [[1.33767742 2.09157724]], train loss = [[0.12715209]]\n",
      "iteration: 25: weight = [[1.32924571 2.1000085 ]], train loss = [[0.12577768]]\n",
      "iteration: 26: weight = [[1.32137602 2.10787764]], train loss = [[0.12458041]]\n",
      "iteration: 27: weight = [[1.31403092 2.11522218]], train loss = [[0.12353744]]\n",
      "iteration: 28: weight = [[1.30717543 2.12207709]], train loss = [[0.1226289]]\n",
      "iteration: 29: weight = [[1.30077694 2.12847502]], train loss = [[0.12183745]]\n",
      "iteration: 30: weight = [[1.29480499 2.13444644]], train loss = [[0.121148]]\n",
      "iteration: 31: weight = [[1.28923114 2.14001979]], train loss = [[0.12054741]]\n",
      "iteration: 32: weight = [[1.28402886 2.14522161]], train loss = [[0.12002422]]\n",
      "iteration: 33: weight = [[1.27917337 2.15007665]], train loss = [[0.11956846]]\n",
      "iteration: 34: weight = [[1.27464157 2.15460805]], train loss = [[0.11917144]]\n",
      "iteration: 35: weight = [[1.27041186 2.15883737]], train loss = [[0.11882559]]\n",
      "iteration: 36: weight = [[1.26646412 2.16278475]], train loss = [[0.11852432]]\n",
      "iteration: 37: weight = [[1.26277955 2.16646899]], train loss = [[0.11826187]]\n",
      "iteration: 38: weight = [[1.2593406  2.16990763]], train loss = [[0.11803325]]\n",
      "iteration: 39: weight = [[1.2561309  2.17311704]], train loss = [[0.11783409]]\n",
      "iteration: 40: weight = [[1.25313516 2.1761125 ]], train loss = [[0.1176606]]\n",
      "iteration: 41: weight = [[1.25033913 2.17890828]], train loss = [[0.11750947]]\n",
      "iteration: 42: weight = [[1.24772949 2.18151768]], train loss = [[0.11737781]]\n",
      "iteration: 43: weight = [[1.24529382 2.18395313]], train loss = [[0.11726313]]\n",
      "iteration: 44: weight = [[1.24302052 2.18622623]], train loss = [[0.11716323]]\n",
      "iteration: 45: weight = [[1.24089876 2.1883478 ]], train loss = [[0.1170762]]\n",
      "iteration: 46: weight = [[1.23891844 2.19032794]], train loss = [[0.11700038]]\n",
      "iteration: 47: weight = [[1.23707013 2.19217607]], train loss = [[0.11693434]]\n",
      "iteration: 48: weight = [[1.23534504 2.19390101]], train loss = [[0.11687681]]\n",
      "iteration: 49: weight = [[1.23373495 2.19551095]], train loss = [[0.1168267]]\n",
      "iteration: 50: weight = [[1.23223219 2.19701358]], train loss = [[0.11678304]]\n",
      "iteration: 51: weight = [[1.23082961 2.19841603]], train loss = [[0.11674501]]\n",
      "iteration: 52: weight = [[1.22952052 2.19972499]], train loss = [[0.11671188]]\n",
      "iteration: 53: weight = [[1.22829871 2.2009467 ]], train loss = [[0.11668302]]\n",
      "iteration: 54: weight = [[1.22715834 2.20208696]], train loss = [[0.11665789]]\n",
      "iteration: 55: weight = [[1.226094   2.20315121]], train loss = [[0.11663599]]\n",
      "iteration: 56: weight = [[1.2251006  2.20414451]], train loss = [[0.11661691]]\n",
      "iteration: 57: weight = [[1.22417343 2.2050716 ]], train loss = [[0.11660029]]\n",
      "iteration: 58: weight = [[1.22330807 2.20593688]], train loss = [[0.11658581]]\n",
      "iteration: 59: weight = [[1.22250039 2.20674449]], train loss = [[0.1165732]]\n",
      "iteration: 60: weight = [[1.22174656 2.20749825]], train loss = [[0.11656222]]\n",
      "iteration: 61: weight = [[1.22104298 2.20820177]], train loss = [[0.11655265]]\n",
      "iteration: 62: weight = [[1.2203863  2.20885839]], train loss = [[0.11654431]]\n",
      "iteration: 63: weight = [[1.2197734  2.20947124]], train loss = [[0.11653705]]\n",
      "iteration: 64: weight = [[1.21920135 2.21004323]], train loss = [[0.11653072]]\n",
      "iteration: 65: weight = [[1.21866744 2.21057709]], train loss = [[0.11652521]]\n",
      "iteration: 66: weight = [[1.21816912 2.21107537]], train loss = [[0.11652041]]\n",
      "iteration: 67: weight = [[1.21770402 2.21154043]], train loss = [[0.11651623]]\n",
      "iteration: 68: weight = [[1.21726993 2.21197448]], train loss = [[0.11651259]]\n",
      "iteration: 69: weight = [[1.21686477 2.2123796 ]], train loss = [[0.11650941]]\n",
      "iteration: 70: weight = [[1.21648662 2.21275772]], train loss = [[0.11650665]]\n",
      "iteration: 71: weight = [[1.21613368 2.21311062]], train loss = [[0.11650424]]\n",
      "iteration: 72: weight = [[1.21580427 2.21344001]], train loss = [[0.11650214]]\n",
      "iteration: 73: weight = [[1.21549682 2.21374743]], train loss = [[0.11650032]]\n",
      "iteration: 74: weight = [[1.21520986 2.21403436]], train loss = [[0.11649873]]\n",
      "iteration: 75: weight = [[1.21494203 2.21430216]], train loss = [[0.11649734]]\n",
      "iteration: 76: weight = [[1.21469206 2.21455211]], train loss = [[0.11649613]]\n",
      "iteration: 77: weight = [[1.21445875 2.2147854 ]], train loss = [[0.11649508]]\n",
      "iteration: 78: weight = [[1.21424099 2.21500314]], train loss = [[0.11649416]]\n",
      "iteration: 79: weight = [[1.21403775 2.21520636]], train loss = [[0.11649336]]\n",
      "iteration: 80: weight = [[1.21384806 2.21539604]], train loss = [[0.11649267]]\n",
      "iteration: 81: weight = [[1.21367101 2.21557307]], train loss = [[0.11649206]]\n",
      "iteration: 82: weight = [[1.21350577 2.2157383 ]], train loss = [[0.11649153]]\n",
      "iteration: 83: weight = [[1.21335154 2.21589251]], train loss = [[0.11649107]]\n",
      "iteration: 84: weight = [[1.21320759 2.21603644]], train loss = [[0.11649067]]\n",
      "iteration: 85: weight = [[1.21307324 2.21617078]], train loss = [[0.11649032]]\n",
      "iteration: 86: weight = [[1.21294785 2.21629617]], train loss = [[0.11649002]]\n",
      "iteration: 87: weight = [[1.21283081 2.21641319]], train loss = [[0.11648976]]\n",
      "iteration: 88: weight = [[1.21272158 2.21652242]], train loss = [[0.11648953]]\n",
      "iteration: 89: weight = [[1.21261962 2.21662436]], train loss = [[0.11648932]]\n",
      "iteration: 90: weight = [[1.21252447 2.21671951]], train loss = [[0.11648915]]\n",
      "iteration: 91: weight = [[1.21243566 2.21680831]], train loss = [[0.116489]]\n",
      "iteration: 92: weight = [[1.21235276 2.21689119]], train loss = [[0.11648886]]\n",
      "iteration: 93: weight = [[1.2122754  2.21696855]], train loss = [[0.11648875]]\n",
      "iteration: 94: weight = [[1.21220319 2.21704076]], train loss = [[0.11648865]]\n",
      "iteration: 95: weight = [[1.21213579 2.21710814]], train loss = [[0.11648856]]\n",
      "iteration: 96: weight = [[1.21207289 2.21717104]], train loss = [[0.11648848]]\n",
      "iteration: 97: weight = [[1.21201418 2.21722974]], train loss = [[0.11648842]]\n",
      "iteration: 98: weight = [[1.21195939 2.21728454]], train loss = [[0.11648836]]\n",
      "iteration: 99: weight = [[1.21190825 2.21733567]], train loss = [[0.11648831]]\n",
      "iteration: 100: weight = [[1.21186051 2.2173834 ]], train loss = [[0.11648826]]\n",
      "iteration: 101: weight = [[1.21181596 2.21742795]], train loss = [[0.11648823]]\n",
      "iteration: 102: weight = [[1.21177438 2.21746953]], train loss = [[0.11648819]]\n",
      "iteration: 103: weight = [[1.21173557 2.21750833]], train loss = [[0.11648816]]\n",
      "iteration: 104: weight = [[1.21169935 2.21754455]], train loss = [[0.11648814]]\n",
      "iteration: 105: weight = [[1.21166554 2.21757836]], train loss = [[0.11648812]]\n",
      "iteration: 106: weight = [[1.21163399 2.21760991]], train loss = [[0.1164881]]\n",
      "iteration: 107: weight = [[1.21160454 2.21763936]], train loss = [[0.11648808]]\n",
      "iteration: 108: weight = [[1.21157705 2.21766684]], train loss = [[0.11648806]]\n",
      "iteration: 109: weight = [[1.21155139 2.21769249]], train loss = [[0.11648805]]\n",
      "iteration: 110: weight = [[1.21152745 2.21771643]], train loss = [[0.11648804]]\n",
      "iteration: 111: weight = [[1.2115051  2.21773878]], train loss = [[0.11648803]]\n",
      "iteration: 112: weight = [[1.21148424 2.21775964]], train loss = [[0.11648802]]\n",
      "iteration: 113: weight = [[1.21146477 2.2177791 ]], train loss = [[0.11648802]]\n",
      "iteration: 114: weight = [[1.2114466  2.21779727]], train loss = [[0.11648801]]\n",
      "iteration: 115: weight = [[1.21142965 2.21781423]], train loss = [[0.116488]]\n",
      "iteration: 116: weight = [[1.21141382 2.21783006]], train loss = [[0.116488]]\n",
      "iteration: 117: weight = [[1.21139904 2.21784483]], train loss = [[0.11648799]]\n",
      "iteration: 118: weight = [[1.21138525 2.21785862]], train loss = [[0.11648799]]\n",
      "iteration: 119: weight = [[1.21137239 2.21787148]], train loss = [[0.11648799]]\n",
      "iteration: 120: weight = [[1.21136037 2.21788349]], train loss = [[0.11648798]]\n",
      "iteration: 121: weight = [[1.21134916 2.2178947 ]], train loss = [[0.11648798]]\n",
      "iteration: 122: weight = [[1.2113387  2.21790517]], train loss = [[0.11648798]]\n",
      "iteration: 123: weight = [[1.21132893 2.21791493]], train loss = [[0.11648798]]\n",
      "iteration: 124: weight = [[1.21131982 2.21792405]], train loss = [[0.11648798]]\n",
      "iteration: 125: weight = [[1.21131131 2.21793255]], train loss = [[0.11648798]]\n",
      "iteration: 126: weight = [[1.21130337 2.21794049]], train loss = [[0.11648797]]\n",
      "iteration: 127: weight = [[1.21129596 2.2179479 ]], train loss = [[0.11648797]]\n",
      "iteration: 128: weight = [[1.21128904 2.21795482]], train loss = [[0.11648797]]\n",
      "iteration: 129: weight = [[1.21128259 2.21796127]], train loss = [[0.11648797]]\n",
      "iteration: 130: weight = [[1.21127656 2.2179673 ]], train loss = [[0.11648797]]\n",
      "iteration: 131: weight = [[1.21127094 2.21797292]], train loss = [[0.11648797]]\n",
      "iteration: 132: weight = [[1.21126569 2.21797817]], train loss = [[0.11648797]]\n",
      "iteration: 133: weight = [[1.21126079 2.21798307]], train loss = [[0.11648797]]\n",
      "iteration: 134: weight = [[1.21125622 2.21798764]], train loss = [[0.11648797]]\n",
      "iteration: 135: weight = [[1.21125195 2.21799191]], train loss = [[0.11648797]]\n",
      "iteration: 136: weight = [[1.21124797 2.21799589]], train loss = [[0.11648797]]\n",
      "iteration: 137: weight = [[1.21124425 2.21799961]], train loss = [[0.11648797]]\n",
      "iteration: 138: weight = [[1.21124078 2.21800308]], train loss = [[0.11648797]]\n",
      "iteration: 139: weight = [[1.21123754 2.21800631]], train loss = [[0.11648797]]\n",
      "iteration: 140: weight = [[1.21123452 2.21800934]], train loss = [[0.11648797]]\n",
      "iteration: 141: weight = [[1.2112317  2.21801216]], train loss = [[0.11648797]]\n",
      "iteration: 142: weight = [[1.21122907 2.21801479]], train loss = [[0.11648797]]\n",
      "iteration: 143: weight = [[1.21122661 2.21801725]], train loss = [[0.11648797]]\n",
      "iteration: 144: weight = [[1.21122432 2.21801954]], train loss = [[0.11648797]]\n",
      "iteration: 145: weight = [[1.21122218 2.21802168]], train loss = [[0.11648797]]\n",
      "iteration: 146: weight = [[1.21122018 2.21802368]], train loss = [[0.11648797]]\n",
      "iteration: 147: weight = [[1.21121831 2.21802554]], train loss = [[0.11648797]]\n",
      "iteration: 148: weight = [[1.21121657 2.21802728]], train loss = [[0.11648797]]\n",
      "iteration: 149: weight = [[1.21121495 2.21802891]], train loss = [[0.11648797]]\n",
      "iteration: 150: weight = [[1.21121343 2.21803042]], train loss = [[0.11648797]]\n",
      "iteration: 151: weight = [[1.21121202 2.21803184]], train loss = [[0.11648797]]\n",
      "iteration: 152: weight = [[1.2112107  2.21803316]], train loss = [[0.11648797]]\n",
      "iteration: 153: weight = [[1.21120946 2.21803439]], train loss = [[0.11648797]]\n",
      "iteration: 154: weight = [[1.21120831 2.21803554]], train loss = [[0.11648797]]\n",
      "iteration: 155: weight = [[1.21120724 2.21803662]], train loss = [[0.11648797]]\n",
      "iteration: 156: weight = [[1.21120624 2.21803762]], train loss = [[0.11648797]]\n",
      "iteration: 157: weight = [[1.2112053  2.21803855]], train loss = [[0.11648797]]\n",
      "iteration: 158: weight = [[1.21120443 2.21803943]], train loss = [[0.11648797]]\n",
      "iteration: 159: weight = [[1.21120361 2.21804024]], train loss = [[0.11648797]]\n",
      "iteration: 160: weight = [[1.21120285 2.218041  ]], train loss = [[0.11648797]]\n",
      "iteration: 161: weight = [[1.21120214 2.21804171]], train loss = [[0.11648797]]\n",
      "iteration: 162: weight = [[1.21120148 2.21804237]], train loss = [[0.11648797]]\n",
      "iteration: 163: weight = [[1.21120086 2.21804299]], train loss = [[0.11648797]]\n",
      "iteration: 164: weight = [[1.21120028 2.21804357]], train loss = [[0.11648797]]\n",
      "iteration: 165: weight = [[1.21119975 2.21804411]], train loss = [[0.11648797]]\n",
      "iteration: 166: weight = [[1.21119924 2.21804461]], train loss = [[0.11648797]]\n",
      "iteration: 167: weight = [[1.21119877 2.21804508]], train loss = [[0.11648797]]\n",
      "iteration: 168: weight = [[1.21119834 2.21804552]], train loss = [[0.11648797]]\n",
      "iteration: 169: weight = [[1.21119793 2.21804593]], train loss = [[0.11648797]]\n",
      "iteration: 170: weight = [[1.21119755 2.21804631]], train loss = [[0.11648797]]\n",
      "iteration: 171: weight = [[1.21119719 2.21804666]], train loss = [[0.11648797]]\n",
      "iteration: 172: weight = [[1.21119686 2.218047  ]], train loss = [[0.11648797]]\n",
      "iteration: 173: weight = [[1.21119655 2.21804731]], train loss = [[0.11648797]]\n",
      "iteration: 174: weight = [[1.21119626 2.2180476 ]], train loss = [[0.11648797]]\n",
      "iteration: 175: weight = [[1.21119599 2.21804787]], train loss = [[0.11648797]]\n",
      "iteration: 176: weight = [[1.21119573 2.21804812]], train loss = [[0.11648797]]\n",
      "iteration: 177: weight = [[1.2111955  2.21804835]], train loss = [[0.11648797]]\n",
      "iteration: 178: weight = [[1.21119528 2.21804857]], train loss = [[0.11648797]]\n",
      "iteration: 179: weight = [[1.21119507 2.21804878]], train loss = [[0.11648797]]\n",
      "iteration: 180: weight = [[1.21119488 2.21804897]], train loss = [[0.11648797]]\n",
      "iteration: 181: weight = [[1.2111947  2.21804915]], train loss = [[0.11648797]]\n",
      "iteration: 182: weight = [[1.21119454 2.21804932]], train loss = [[0.11648797]]\n",
      "iteration: 183: weight = [[1.21119438 2.21804947]], train loss = [[0.11648797]]\n",
      "iteration: 184: weight = [[1.21119424 2.21804962]], train loss = [[0.11648797]]\n",
      "iteration: 185: weight = [[1.2111941  2.21804975]], train loss = [[0.11648797]]\n",
      "iteration: 186: weight = [[1.21119397 2.21804988]], train loss = [[0.11648797]]\n",
      "iteration: 187: weight = [[1.21119386 2.21805   ]], train loss = [[0.11648797]]\n",
      "iteration: 188: weight = [[1.21119375 2.21805011]], train loss = [[0.11648797]]\n",
      "iteration: 189: weight = [[1.21119364 2.21805021]], train loss = [[0.11648797]]\n",
      "iteration: 190: weight = [[1.21119355 2.21805031]], train loss = [[0.11648797]]\n",
      "iteration: 191: weight = [[1.21119346 2.2180504 ]], train loss = [[0.11648797]]\n",
      "iteration: 192: weight = [[1.21119337 2.21805048]], train loss = [[0.11648797]]\n",
      "iteration: 193: weight = [[1.2111933  2.21805056]], train loss = [[0.11648797]]\n",
      "iteration: 194: weight = [[1.21119322 2.21805063]], train loss = [[0.11648797]]\n",
      "iteration: 195: weight = [[1.21119316 2.2180507 ]], train loss = [[0.11648797]]\n",
      "iteration: 196: weight = [[1.21119309 2.21805076]], train loss = [[0.11648797]]\n",
      "iteration: 197: weight = [[1.21119303 2.21805082]], train loss = [[0.11648797]]\n",
      "iteration: 198: weight = [[1.21119298 2.21805088]], train loss = [[0.11648797]]\n",
      "iteration: 199: weight = [[1.21119293 2.21805093]], train loss = [[0.11648797]]\n",
      "iteration: 200: weight = [[1.21119288 2.21805098]], train loss = [[0.11648797]]\n",
      "iteration: 201: weight = [[1.21119283 2.21805102]], train loss = [[0.11648797]]\n",
      "iteration: 202: weight = [[1.21119279 2.21805106]], train loss = [[0.11648797]]\n",
      "iteration: 203: weight = [[1.21119275 2.2180511 ]], train loss = [[0.11648797]]\n",
      "iteration: 204: weight = [[1.21119272 2.21805114]], train loss = [[0.11648797]]\n",
      "iteration: 205: weight = [[1.21119268 2.21805117]], train loss = [[0.11648797]]\n",
      "iteration: 206: weight = [[1.21119265 2.2180512 ]], train loss = [[0.11648797]]\n",
      "iteration: 207: weight = [[1.21119262 2.21805123]], train loss = [[0.11648797]]\n",
      "iteration: 208: weight = [[1.21119259 2.21805126]], train loss = [[0.11648797]]\n",
      "iteration: 209: weight = [[1.21119257 2.21805129]], train loss = [[0.11648797]]\n",
      "iteration: 210: weight = [[1.21119254 2.21805131]], train loss = [[0.11648797]]\n",
      "iteration: 211: weight = [[1.21119252 2.21805133]], train loss = [[0.11648797]]\n",
      "iteration: 212: weight = [[1.2111925  2.21805136]], train loss = [[0.11648797]]\n",
      "iteration: 213: weight = [[1.21119248 2.21805138]], train loss = [[0.11648797]]\n",
      "iteration: 214: weight = [[1.21119246 2.21805139]], train loss = [[0.11648797]]\n",
      "iteration: 215: weight = [[1.21119244 2.21805141]], train loss = [[0.11648797]]\n",
      "iteration: 216: weight = [[1.21119243 2.21805143]], train loss = [[0.11648797]]\n",
      "iteration: 217: weight = [[1.21119241 2.21805144]], train loss = [[0.11648797]]\n",
      "iteration: 218: weight = [[1.2111924  2.21805146]], train loss = [[0.11648797]]\n",
      "iteration: 219: weight = [[1.21119239 2.21805147]], train loss = [[0.11648797]]\n",
      "iteration: 220: weight = [[1.21119237 2.21805148]], train loss = [[0.11648797]]\n",
      "iteration: 221: weight = [[1.21119236 2.21805149]], train loss = [[0.11648797]]\n",
      "iteration: 222: weight = [[1.21119235 2.2180515 ]], train loss = [[0.11648797]]\n",
      "iteration: 223: weight = [[1.21119234 2.21805151]], train loss = [[0.11648797]]\n",
      "iteration: 224: weight = [[1.21119233 2.21805152]], train loss = [[0.11648797]]\n",
      "iteration: 225: weight = [[1.21119232 2.21805153]], train loss = [[0.11648797]]\n",
      "iteration: 226: weight = [[1.21119232 2.21805154]], train loss = [[0.11648797]]\n",
      "iteration: 227: weight = [[1.21119231 2.21805155]], train loss = [[0.11648797]]\n",
      "iteration: 228: weight = [[1.2111923  2.21805155]], train loss = [[0.11648797]]\n",
      "iteration: 229: weight = [[1.21119229 2.21805156]], train loss = [[0.11648797]]\n",
      "iteration: 230: weight = [[1.21119229 2.21805157]], train loss = [[0.11648797]]\n",
      "iteration: 231: weight = [[1.21119228 2.21805157]], train loss = [[0.11648797]]\n",
      "iteration: 232: weight = [[1.21119228 2.21805158]], train loss = [[0.11648797]]\n",
      "iteration: 233: weight = [[1.21119227 2.21805158]], train loss = [[0.11648797]]\n",
      "iteration: 234: weight = [[1.21119227 2.21805159]], train loss = [[0.11648797]]\n",
      "iteration: 235: weight = [[1.21119226 2.21805159]], train loss = [[0.11648797]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 236: weight = [[1.21119226 2.21805159]], train loss = [[0.11648797]]\n",
      "iteration: 237: weight = [[1.21119226 2.2180516 ]], train loss = [[0.11648797]]\n",
      "iteration: 238: weight = [[1.21119225 2.2180516 ]], train loss = [[0.11648797]]\n",
      "iteration: 239: weight = [[1.21119225 2.2180516 ]], train loss = [[0.11648797]]\n",
      "iteration: 240: weight = [[1.21119225 2.21805161]], train loss = [[0.11648797]]\n",
      "iteration: 241: weight = [[1.21119224 2.21805161]], train loss = [[0.11648797]]\n",
      "iteration: 242: weight = [[1.21119224 2.21805161]], train loss = [[0.11648797]]\n",
      "iteration: 243: weight = [[1.21119224 2.21805162]], train loss = [[0.11648797]]\n",
      "iteration: 244: weight = [[1.21119224 2.21805162]], train loss = [[0.11648797]]\n",
      "iteration: 245: weight = [[1.21119223 2.21805162]], train loss = [[0.11648797]]\n",
      "iteration: 246: weight = [[1.21119223 2.21805162]], train loss = [[0.11648797]]\n",
      "iteration: 247: weight = [[1.21119223 2.21805162]], train loss = [[0.11648797]]\n",
      "iteration: 248: weight = [[1.21119223 2.21805163]], train loss = [[0.11648797]]\n",
      "iteration: 249: weight = [[1.21119223 2.21805163]], train loss = [[0.11648797]]\n",
      "iteration: 250: weight = [[1.21119222 2.21805163]], train loss = [[0.11648797]]\n",
      "iteration: 251: weight = [[1.21119222 2.21805163]], train loss = [[0.11648797]]\n",
      "iteration: 252: weight = [[1.21119222 2.21805163]], train loss = [[0.11648797]]\n",
      "iteration: 253: weight = [[1.21119222 2.21805163]], train loss = [[0.11648797]]\n",
      "iteration: 254: weight = [[1.21119222 2.21805163]], train loss = [[0.11648797]]\n",
      "iteration: 255: weight = [[1.21119222 2.21805163]], train loss = [[0.11648797]]\n",
      "iteration: 256: weight = [[1.21119222 2.21805164]], train loss = [[0.11648797]]\n",
      "iteration: 257: weight = [[1.21119222 2.21805164]], train loss = [[0.11648797]]\n",
      "iteration: 258: weight = [[1.21119222 2.21805164]], train loss = [[0.11648797]]\n",
      "iteration: 259: weight = [[1.21119222 2.21805164]], train loss = [[0.11648797]]\n",
      "iteration: 260: weight = [[1.21119221 2.21805164]], train loss = [[0.11648797]]\n",
      "iteration: 261: weight = [[1.21119221 2.21805164]], train loss = [[0.11648797]]\n",
      "iteration: 262: weight = [[1.21119221 2.21805164]], train loss = [[0.11648797]]\n",
      "iteration: 263: weight = [[1.21119221 2.21805164]], train loss = [[0.11648797]]\n",
      "iteration: 264: weight = [[1.21119221 2.21805164]], train loss = [[0.11648797]]\n",
      "iteration: 265: weight = [[1.21119221 2.21805164]], train loss = [[0.11648797]]\n",
      "iteration: 266: weight = [[1.21119221 2.21805164]], train loss = [[0.11648797]]\n",
      "iteration: 267: weight = [[1.21119221 2.21805164]], train loss = [[0.11648797]]\n",
      "iteration: 268: weight = [[1.21119221 2.21805164]], train loss = [[0.11648797]]\n",
      "iteration: 269: weight = [[1.21119221 2.21805164]], train loss = [[0.11648797]]\n",
      "iteration: 270: weight = [[1.21119221 2.21805164]], train loss = [[0.11648797]]\n",
      "iteration: 271: weight = [[1.21119221 2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 272: weight = [[1.21119221 2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 273: weight = [[1.21119221 2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 274: weight = [[1.21119221 2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 275: weight = [[1.21119221 2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 276: weight = [[1.21119221 2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 277: weight = [[1.21119221 2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 278: weight = [[1.21119221 2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 279: weight = [[1.21119221 2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 280: weight = [[1.21119221 2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 281: weight = [[1.21119221 2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 282: weight = [[1.21119221 2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 283: weight = [[1.21119221 2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 284: weight = [[1.21119221 2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 285: weight = [[1.21119221 2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 286: weight = [[1.21119221 2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 287: weight = [[1.21119221 2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 288: weight = [[1.21119221 2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 289: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 290: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 291: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 292: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 293: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 294: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 295: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 296: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 297: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 298: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 299: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 300: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 301: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 302: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 303: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 304: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 305: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 306: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 307: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 308: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 309: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 310: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 311: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 312: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 313: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 314: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 315: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 316: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 317: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 318: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 319: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 320: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 321: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 322: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 323: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 324: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 325: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 326: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 327: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 328: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 329: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 330: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 331: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 332: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 333: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 334: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 335: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 336: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 337: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 338: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 339: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 340: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 341: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 342: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 343: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 344: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 345: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 346: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 347: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 348: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 349: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 350: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 351: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 352: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 353: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 354: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 355: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 356: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 357: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 358: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 359: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 360: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 361: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 362: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 363: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 364: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 365: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 366: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 367: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 368: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 369: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 370: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 371: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 372: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 373: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 374: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 375: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 376: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 377: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 378: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 379: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 380: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 381: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 382: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 383: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 384: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 385: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 386: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 387: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 388: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 389: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 390: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 391: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 392: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 393: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 394: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 395: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 396: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 397: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 398: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 399: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 400: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 401: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 402: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 403: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 404: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 405: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 406: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 407: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 408: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 409: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 410: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 411: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 412: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 413: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 414: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 415: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 416: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 417: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 418: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 419: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 420: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 421: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 422: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 423: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 424: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 425: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 426: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 427: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 428: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 429: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 430: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 431: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 432: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 433: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 434: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 435: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 436: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 437: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 438: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 439: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 440: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 441: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 442: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 443: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 444: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 445: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 446: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 447: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 448: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 449: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 450: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 451: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 452: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 453: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 454: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 455: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 456: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 457: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 458: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 459: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 460: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 461: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 462: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 463: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 464: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 465: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 466: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 467: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 468: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 469: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 470: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 471: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 472: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 473: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 474: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 475: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 476: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 477: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 478: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 479: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 480: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 481: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 482: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 483: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 484: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 485: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 486: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 487: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 488: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 489: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 490: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 491: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 492: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 493: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 494: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 495: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 496: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 497: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 498: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 499: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 500: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 501: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 502: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 503: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 504: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 505: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 506: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 507: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 508: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 509: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 510: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 511: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 512: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 513: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 514: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 515: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 516: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 517: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 518: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 519: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 520: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 521: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 522: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 523: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 524: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 525: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 526: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 527: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 528: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 529: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 530: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 531: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 532: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 533: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 534: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 535: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 536: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 537: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 538: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 539: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 540: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 541: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 542: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 543: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 544: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 545: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 546: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 547: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 548: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 549: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 550: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 551: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 552: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 553: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 554: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 555: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 556: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 557: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 558: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 559: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 560: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 561: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 562: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 563: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 564: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 565: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 566: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 567: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 568: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 569: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 570: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 571: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 572: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 573: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 574: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 575: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 576: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 577: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 578: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 579: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 580: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 581: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 582: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 583: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 584: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 585: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 586: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 587: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 588: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 589: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 590: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 591: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 592: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 593: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 594: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 595: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 596: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 597: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 598: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 599: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 600: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 601: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 602: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 603: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 604: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 605: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 606: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 607: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 608: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 609: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 610: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 611: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 612: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 613: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 614: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 615: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 616: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 617: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 618: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 619: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 620: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 621: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 622: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 623: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 624: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 625: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 626: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 627: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 628: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 629: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 630: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 631: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 632: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 633: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 634: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 635: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 636: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 637: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 638: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 639: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 640: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 641: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 642: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 643: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 644: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 645: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 646: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 647: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 648: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 649: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 650: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 651: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 652: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 653: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 654: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 655: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 656: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 657: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 658: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 659: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 660: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 661: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 662: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 663: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 664: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 665: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 666: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 667: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 668: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 669: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 670: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 671: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 672: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 673: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 674: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 675: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 676: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 677: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 678: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 679: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 680: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 681: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 682: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 683: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 684: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 685: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 686: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 687: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 688: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 689: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 690: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 691: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 692: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 693: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 694: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 695: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 696: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 697: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 698: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 699: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 700: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 701: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 702: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 703: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 704: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 705: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 706: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 707: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 708: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 709: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 710: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 711: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 712: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 713: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 714: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 715: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 716: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 717: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 718: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 719: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 720: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 721: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 722: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 723: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 724: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 725: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 726: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 727: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 728: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 729: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 730: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 731: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 732: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 733: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 734: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 735: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 736: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 737: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 738: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 739: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 740: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 741: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 742: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 743: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 744: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 745: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 746: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 747: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 748: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 749: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 750: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 751: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 752: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 753: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 754: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 755: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 756: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 757: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 758: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 759: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 760: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 761: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 762: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 763: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 764: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 765: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 766: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 767: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 768: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 769: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 770: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 771: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 772: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 773: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 774: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 775: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 776: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 777: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 778: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 779: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 780: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 781: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 782: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 783: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 784: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 785: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 786: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 787: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 788: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 789: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 790: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 791: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 792: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 793: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 794: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 795: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 796: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 797: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 798: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 799: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 800: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 801: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 802: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 803: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 804: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 805: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 806: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 807: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 808: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 809: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 810: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 811: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 812: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 813: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 814: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 815: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 816: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 817: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 818: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 819: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 820: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 821: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 822: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 823: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 824: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 825: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 826: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 827: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 828: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 829: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 830: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 831: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 832: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 833: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 834: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 835: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 836: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 837: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 838: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 839: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 840: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 841: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 842: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 843: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 844: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 845: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 846: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 847: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 848: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 849: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 850: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 851: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 852: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 853: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 854: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 855: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 856: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 857: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 858: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 859: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 860: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 861: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 862: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 863: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 864: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 865: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 866: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 867: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 868: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 869: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 870: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 871: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 872: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 873: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 874: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 875: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 876: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 877: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 878: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 879: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 880: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 881: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 882: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 883: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 884: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 885: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 886: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 887: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 888: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 889: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 890: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 891: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 892: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 893: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 894: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 895: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 896: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 897: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 898: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 899: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 900: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 901: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 902: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 903: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 904: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 905: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 906: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 907: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 908: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 909: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 910: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 911: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 912: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 913: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 914: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 915: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 916: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 917: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 918: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 919: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 920: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 921: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 922: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 923: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 924: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 925: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 926: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 927: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 928: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 929: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 930: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 931: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 932: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 933: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 934: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 935: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 936: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 937: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 938: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 939: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 940: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 941: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 942: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 943: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 944: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 945: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 946: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 947: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 948: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 949: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 950: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 951: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 952: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 953: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 954: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 955: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 956: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 957: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 958: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 959: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 960: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 961: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 962: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 963: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 964: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 965: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 966: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 967: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 968: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 969: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 970: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 971: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 972: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 973: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 974: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 975: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 976: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 977: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 978: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 979: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 980: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 981: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 982: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 983: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 984: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 985: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 986: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 987: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 988: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 989: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 990: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 991: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 992: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 993: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 994: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 995: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 996: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 997: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 998: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 999: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "iteration: 1000: weight = [[1.2111922  2.21805165]], train loss = [[0.11648797]]\n",
      "the time used for optimization: 0.8250932693481445\n"
     ]
    }
   ],
   "source": [
    "X_new = 2 * np.random.rand(10000, 2)\n",
    "y_new = X_new.dot(np.array([[1], [2]])) + np.random.rand(10000, 1)\n",
    "\n",
    "eta = 0.1 #Learning rate\n",
    "n_iter = 1000 #Number of iterations for weight updates\n",
    "d_train = 10000 #Number of training samples\n",
    "\n",
    "w = np.random.rand(1, 2) #Weight vector\n",
    "t1 = time.time()\n",
    "for i in range(n_iter):\n",
    "    gradient = 2/d_train * (w.dot(X_new.T) - y_new.T).dot(X_new)\n",
    "    w = w - eta * gradient\n",
    "    train_loss = 1/d_train * (w.dot(X_new.T) - y_new.T).dot((w.dot(X_new.T) - y_new.T).T)\n",
    "    print('iteration: {}: weight = {}, train loss = {}'.format(i + 1, w, train_loss))\n",
    "t2 = time.time()\n",
    "print('the time used for optimization: {}'.format(t2 - t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826e757d",
   "metadata": {},
   "source": [
    "# Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d87d9be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 2: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 3: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 4: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 5: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 6: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 7: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 8: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 9: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 10: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 11: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 12: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 13: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 14: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 15: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 16: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 17: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 18: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 19: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 20: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 21: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 22: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 23: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 24: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 25: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 26: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 27: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 28: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 29: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 30: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 31: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 32: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 33: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 34: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 35: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 36: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 37: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 38: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 39: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 40: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 41: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 42: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 43: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 44: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 45: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 46: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 47: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 48: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 49: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 50: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 51: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 52: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 53: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 54: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 55: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 56: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 57: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 58: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 59: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 60: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 61: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 62: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 63: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 64: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 65: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 66: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 67: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 68: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 69: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 70: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 71: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 72: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 73: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 74: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 75: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 76: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 77: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 78: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 79: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 80: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 81: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 82: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 83: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 84: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 85: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 86: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 87: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 88: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 89: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 90: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 91: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 92: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 93: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 94: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 95: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 96: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 97: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 98: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 99: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 100: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 101: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 102: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 103: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 104: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 105: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 106: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 107: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 108: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 109: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 110: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 111: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 112: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 113: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 114: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 115: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 116: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 117: weight = [3.36535747], train loss = 4.5436766555169624e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 118: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 119: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 120: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 121: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 122: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 123: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 124: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 125: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 126: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 127: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 128: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 129: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 130: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 131: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 132: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 133: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 134: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 135: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 136: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 137: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 138: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 139: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 140: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 141: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 142: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 143: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 144: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 145: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 146: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 147: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 148: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 149: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 150: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 151: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 152: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 153: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 154: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 155: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 156: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 157: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 158: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 159: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 160: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 161: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 162: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 163: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 164: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 165: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 166: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 167: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 168: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 169: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 170: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 171: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 172: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 173: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 174: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 175: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 176: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 177: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 178: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 179: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 180: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 181: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 182: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 183: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 184: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 185: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 186: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 187: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 188: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 189: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 190: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 191: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 192: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 193: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 194: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 195: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 196: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 197: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 198: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 199: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 200: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 201: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 202: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 203: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 204: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 205: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 206: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 207: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 208: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 209: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 210: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 211: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 212: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 213: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 214: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 215: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 216: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 217: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 218: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 219: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 220: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 221: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 222: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 223: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 224: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 225: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 226: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 227: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 228: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 229: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 230: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 231: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 232: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 233: weight = [3.36535747], train loss = 4.5436766555169624e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 234: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 235: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 236: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 237: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 238: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 239: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 240: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 241: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 242: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 243: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 244: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 245: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 246: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 247: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 248: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 249: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 250: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 251: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 252: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 253: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 254: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 255: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 256: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 257: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 258: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 259: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 260: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 261: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 262: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 263: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 264: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 265: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 266: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 267: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 268: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 269: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 270: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 271: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 272: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 273: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 274: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 275: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 276: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 277: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 278: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 279: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 280: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 281: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 282: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 283: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 284: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 285: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 286: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 287: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 288: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 289: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 290: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 291: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 292: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 293: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 294: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 295: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 296: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 297: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 298: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 299: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 300: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 301: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 302: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 303: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 304: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 305: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 306: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 307: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 308: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 309: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 310: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 311: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 312: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 313: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 314: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 315: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 316: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 317: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 318: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 319: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 320: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 321: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 322: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 323: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 324: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 325: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 326: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 327: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 328: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 329: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 330: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 331: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 332: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 333: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 334: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 335: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 336: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 337: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 338: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 339: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 340: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 341: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 342: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 343: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 344: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 345: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 346: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 347: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 348: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 349: weight = [3.36535747], train loss = 4.5436766555169624e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 350: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 351: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 352: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 353: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 354: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 355: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 356: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 357: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 358: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 359: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 360: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 361: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 362: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 363: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 364: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 365: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 366: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 367: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 368: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 369: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 370: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 371: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 372: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 373: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 374: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 375: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 376: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 377: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 378: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 379: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 380: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 381: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 382: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 383: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 384: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 385: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 386: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 387: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 388: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 389: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 390: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 391: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 392: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 393: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 394: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 395: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 396: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 397: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 398: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 399: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 400: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 401: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 402: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 403: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 404: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 405: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 406: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 407: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 408: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 409: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 410: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 411: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 412: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 413: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 414: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 415: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 416: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 417: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 418: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 419: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 420: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 421: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 422: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 423: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 424: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 425: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 426: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 427: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 428: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 429: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 430: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 431: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 432: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 433: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 434: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 435: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 436: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 437: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 438: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 439: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 440: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 441: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 442: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 443: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 444: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 445: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 446: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 447: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 448: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 449: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 450: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 451: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 452: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 453: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 454: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 455: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 456: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 457: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 458: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 459: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 460: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 461: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 462: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 463: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 464: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 465: weight = [3.36535747], train loss = 4.5436766555169624e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 466: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 467: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 468: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 469: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 470: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 471: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 472: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 473: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 474: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 475: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 476: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 477: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 478: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 479: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 480: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 481: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 482: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 483: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 484: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 485: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 486: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 487: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 488: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 489: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 490: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 491: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 492: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 493: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 494: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 495: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 496: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 497: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 498: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 499: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 500: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 501: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 502: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 503: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 504: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 505: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 506: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 507: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 508: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 509: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 510: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 511: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 512: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 513: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 514: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 515: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 516: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 517: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 518: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 519: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 520: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 521: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 522: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 523: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 524: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 525: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 526: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 527: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 528: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 529: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 530: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 531: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 532: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 533: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 534: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 535: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 536: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 537: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 538: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 539: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 540: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 541: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 542: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 543: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 544: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 545: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 546: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 547: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 548: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 549: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 550: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 551: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 552: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 553: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 554: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 555: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 556: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 557: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 558: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 559: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 560: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 561: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 562: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 563: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 564: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 565: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 566: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 567: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 568: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 569: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 570: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 571: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 572: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 573: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 574: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 575: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 576: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 577: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 578: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 579: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 580: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 581: weight = [3.36535747], train loss = 4.5436766555169624e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 582: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 583: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 584: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 585: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 586: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 587: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 588: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 589: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 590: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 591: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 592: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 593: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 594: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 595: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 596: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 597: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 598: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 599: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 600: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 601: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 602: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 603: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 604: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 605: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 606: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 607: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 608: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 609: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 610: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 611: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 612: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 613: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 614: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 615: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 616: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 617: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 618: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 619: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 620: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 621: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 622: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 623: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 624: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 625: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 626: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 627: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 628: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 629: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 630: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 631: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 632: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 633: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 634: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 635: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 636: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 637: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 638: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 639: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 640: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 641: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 642: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 643: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 644: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 645: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 646: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 647: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 648: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 649: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 650: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 651: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 652: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 653: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 654: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 655: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 656: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 657: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 658: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 659: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 660: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 661: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 662: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 663: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 664: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 665: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 666: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 667: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 668: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 669: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 670: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 671: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 672: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 673: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 674: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 675: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 676: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 677: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 678: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 679: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 680: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 681: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 682: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 683: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 684: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 685: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 686: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 687: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 688: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 689: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 690: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 691: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 692: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 693: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 694: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 695: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 696: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 697: weight = [3.36535747], train loss = 4.5436766555169624e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 698: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 699: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 700: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 701: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 702: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 703: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 704: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 705: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 706: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 707: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 708: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 709: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 710: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 711: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 712: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 713: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 714: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 715: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 716: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 717: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 718: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 719: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 720: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 721: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 722: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 723: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 724: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 725: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 726: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 727: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 728: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 729: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 730: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 731: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 732: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 733: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 734: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 735: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 736: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 737: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 738: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 739: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 740: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 741: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 742: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 743: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 744: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 745: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 746: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 747: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 748: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 749: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 750: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 751: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 752: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 753: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 754: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 755: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 756: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 757: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 758: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 759: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 760: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 761: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 762: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 763: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 764: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 765: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 766: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 767: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 768: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 769: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 770: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 771: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 772: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 773: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 774: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 775: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 776: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 777: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 778: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 779: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 780: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 781: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 782: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 783: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 784: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 785: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 786: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 787: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 788: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 789: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 790: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 791: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 792: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 793: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 794: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 795: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 796: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 797: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 798: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 799: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 800: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 801: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 802: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 803: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 804: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 805: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 806: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 807: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 808: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 809: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 810: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 811: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 812: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 813: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 814: weight = [3.36535747], train loss = 4.5436766555169624e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 815: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 816: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 817: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 818: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 819: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 820: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 821: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 822: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 823: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 824: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 825: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 826: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 827: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 828: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 829: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 830: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 831: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 832: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 833: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 834: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 835: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 836: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 837: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 838: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 839: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 840: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 841: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 842: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 843: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 844: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 845: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 846: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 847: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 848: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 849: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 850: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 851: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 852: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 853: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 854: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 855: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 856: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 857: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 858: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 859: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 860: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 861: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 862: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 863: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 864: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 865: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 866: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 867: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 868: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 869: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 870: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 871: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 872: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 873: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 874: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 875: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 876: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 877: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 878: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 879: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 880: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 881: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 882: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 883: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 884: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 885: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 886: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 887: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 888: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 889: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 890: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 891: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 892: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 893: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 894: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 895: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 896: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 897: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 898: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 899: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 900: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 901: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 902: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 903: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 904: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 905: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 906: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 907: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 908: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 909: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 910: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 911: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 912: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 913: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 914: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 915: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 916: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 917: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 918: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 919: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 920: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 921: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 922: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 923: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 924: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 925: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 926: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 927: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 928: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 929: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 930: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 931: weight = [3.36535747], train loss = 4.5436766555169624e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 932: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 933: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 934: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 935: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 936: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 937: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 938: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 939: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 940: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 941: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 942: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 943: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 944: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 945: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 946: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 947: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 948: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 949: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 950: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 951: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 952: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 953: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 954: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 955: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 956: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 957: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 958: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 959: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 960: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 961: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 962: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 963: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 964: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 965: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 966: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 967: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 968: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 969: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 970: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 971: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 972: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 973: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 974: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 975: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 976: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 977: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 978: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 979: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 980: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 981: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 982: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 983: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 984: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 985: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 986: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 987: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 988: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 989: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 990: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 991: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 992: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 993: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 994: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 995: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 996: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 997: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 998: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 999: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "epoch: 1000: weight = [3.36535747], train loss = 4.5436766555169624e-05\n",
      "the time used for optimization: 141.64706373214722\n"
     ]
    }
   ],
   "source": [
    "X_new = 2 * np.random.rand(10000, 1)\n",
    "y_new = 3 * X_new + np.random.rand(10000, 1)\n",
    "\n",
    "eta = 0.1 #Learning rate\n",
    "n_epochs = 1000 #Number of epochs for weight updates\n",
    "d_train = 10000 #Number of training samples\n",
    "\n",
    "w = np.random.rand(1) #Weight vector\n",
    "\n",
    "t1 = time.time()\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(d_train):\n",
    "        xi = X_new[i]\n",
    "        yi = y_new[i]\n",
    "        gradient = 2 * (w.dot(xi.T) - yi.T).dot(xi)\n",
    "        w = w - eta * gradient\n",
    "        train_loss = (w.dot(xi.T) - yi.T).dot((w.dot(xi.T) - yi.T).T)\n",
    "    print('epoch: {}: weight = {}, train loss = {}'.format(epoch + 1, w, train_loss))\n",
    "t2 = time.time()\n",
    "print('the time used for optimization: {}'.format(t2 - t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bcb2a01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_predict = X_new.dot(w.T)\n",
    "plt.plot(X_new, y_new, \"b.\")\n",
    "plt.plot(X_new, y_predict, \"r-\")\n",
    "plt.xlabel(\"X\", fontsize=18)\n",
    "plt.ylabel(\"y\", fontsize=14, rotation=0)\n",
    "plt.axis([0, 2, 0, 10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c9fbd6",
   "metadata": {},
   "source": [
    "# Mini-batch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc34a72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 2: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 3: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 4: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 5: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 6: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 7: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 8: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 9: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 10: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 11: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 12: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 13: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 14: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 15: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 16: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 17: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 18: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 19: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 20: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 21: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 22: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 23: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 24: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 25: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 26: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 27: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 28: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 29: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 30: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 31: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 32: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 33: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 34: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 35: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 36: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 37: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 38: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 39: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 40: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 41: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 42: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 43: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 44: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 45: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 46: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 47: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 48: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 49: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 50: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 51: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 52: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 53: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 54: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 55: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 56: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 57: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 58: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 59: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 60: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 61: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 62: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 63: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 64: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 65: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 66: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 67: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 68: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 69: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 70: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 71: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 72: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 73: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 74: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 75: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 76: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 77: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 78: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 79: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 80: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 81: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 82: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 83: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 84: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 85: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 86: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 87: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 88: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 89: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 90: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 91: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 92: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 93: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 94: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 95: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 96: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 97: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 98: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 99: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 100: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 101: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 102: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 103: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 104: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 105: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 106: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 107: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 108: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 109: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 110: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 111: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 112: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 113: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 114: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 115: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 116: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 117: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 118: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 119: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 120: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 121: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 122: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 123: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 124: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 125: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 126: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 127: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 128: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 129: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 130: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 131: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 132: weight = [[3.35791339]], train loss = [[0.18409348]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 133: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 134: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 135: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 136: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 137: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 138: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 139: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 140: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 141: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 142: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 143: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 144: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 145: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 146: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 147: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 148: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 149: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 150: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 151: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 152: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 153: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 154: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 155: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 156: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 157: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 158: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 159: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 160: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 161: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 162: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 163: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 164: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 165: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 166: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 167: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 168: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 169: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 170: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 171: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 172: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 173: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 174: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 175: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 176: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 177: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 178: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 179: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 180: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 181: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 182: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 183: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 184: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 185: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 186: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 187: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 188: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 189: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 190: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 191: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 192: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 193: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 194: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 195: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 196: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 197: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 198: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 199: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 200: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 201: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 202: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 203: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 204: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 205: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 206: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 207: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 208: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 209: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 210: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 211: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 212: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 213: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 214: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 215: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 216: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 217: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 218: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 219: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 220: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 221: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 222: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 223: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 224: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 225: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 226: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 227: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 228: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 229: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 230: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 231: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 232: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 233: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 234: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 235: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 236: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 237: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 238: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 239: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 240: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 241: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 242: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 243: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 244: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 245: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 246: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 247: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 248: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 249: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 250: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 251: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 252: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 253: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 254: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 255: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 256: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 257: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 258: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 259: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 260: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 261: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 262: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 263: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 264: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 265: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 266: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 267: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 268: weight = [[3.35791339]], train loss = [[0.18409348]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 269: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 270: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 271: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 272: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 273: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 274: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 275: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 276: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 277: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 278: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 279: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 280: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 281: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 282: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 283: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 284: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 285: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 286: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 287: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 288: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 289: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 290: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 291: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 292: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 293: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 294: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 295: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 296: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 297: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 298: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 299: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 300: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 301: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 302: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 303: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 304: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 305: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 306: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 307: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 308: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 309: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 310: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 311: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 312: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 313: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 314: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 315: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 316: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 317: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 318: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 319: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 320: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 321: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 322: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 323: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 324: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 325: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 326: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 327: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 328: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 329: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 330: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 331: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 332: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 333: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 334: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 335: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 336: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 337: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 338: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 339: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 340: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 341: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 342: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 343: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 344: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 345: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 346: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 347: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 348: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 349: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 350: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 351: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 352: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 353: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 354: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 355: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 356: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 357: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 358: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 359: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 360: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 361: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 362: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 363: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 364: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 365: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 366: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 367: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 368: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 369: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 370: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 371: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 372: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 373: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 374: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 375: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 376: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 377: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 378: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 379: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 380: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 381: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 382: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 383: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 384: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 385: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 386: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 387: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 388: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 389: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 390: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 391: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 392: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 393: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 394: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 395: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 396: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 397: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 398: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 399: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 400: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 401: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 402: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 403: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 404: weight = [[3.35791339]], train loss = [[0.18409348]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 405: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 406: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 407: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 408: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 409: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 410: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 411: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 412: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 413: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 414: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 415: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 416: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 417: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 418: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 419: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 420: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 421: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 422: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 423: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 424: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 425: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 426: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 427: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 428: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 429: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 430: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 431: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 432: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 433: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 434: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 435: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 436: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 437: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 438: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 439: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 440: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 441: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 442: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 443: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 444: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 445: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 446: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 447: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 448: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 449: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 450: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 451: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 452: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 453: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 454: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 455: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 456: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 457: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 458: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 459: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 460: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 461: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 462: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 463: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 464: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 465: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 466: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 467: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 468: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 469: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 470: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 471: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 472: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 473: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 474: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 475: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 476: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 477: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 478: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 479: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 480: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 481: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 482: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 483: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 484: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 485: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 486: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 487: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 488: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 489: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 490: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 491: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 492: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 493: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 494: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 495: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 496: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 497: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 498: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 499: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 500: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 501: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 502: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 503: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 504: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 505: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 506: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 507: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 508: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 509: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 510: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 511: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 512: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 513: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 514: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 515: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 516: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 517: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 518: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 519: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 520: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 521: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 522: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 523: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 524: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 525: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 526: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 527: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 528: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 529: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 530: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 531: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 532: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 533: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 534: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 535: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 536: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 537: weight = [[3.35791339]], train loss = [[0.18409348]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 538: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 539: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 540: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 541: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 542: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 543: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 544: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 545: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 546: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 547: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 548: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 549: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 550: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 551: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 552: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 553: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 554: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 555: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 556: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 557: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 558: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 559: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 560: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 561: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 562: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 563: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 564: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 565: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 566: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 567: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 568: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 569: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 570: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 571: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 572: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 573: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 574: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 575: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 576: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 577: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 578: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 579: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 580: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 581: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 582: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 583: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 584: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 585: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 586: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 587: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 588: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 589: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 590: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 591: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 592: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 593: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 594: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 595: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 596: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 597: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 598: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 599: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 600: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 601: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 602: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 603: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 604: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 605: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 606: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 607: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 608: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 609: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 610: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 611: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 612: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 613: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 614: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 615: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 616: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 617: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 618: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 619: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 620: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 621: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 622: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 623: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 624: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 625: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 626: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 627: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 628: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 629: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 630: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 631: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 632: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 633: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 634: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 635: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 636: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 637: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 638: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 639: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 640: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 641: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 642: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 643: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 644: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 645: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 646: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 647: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 648: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 649: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 650: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 651: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 652: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 653: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 654: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 655: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 656: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 657: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 658: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 659: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 660: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 661: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 662: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 663: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 664: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 665: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 666: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 667: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 668: weight = [[3.35791339]], train loss = [[0.18409348]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 669: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 670: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 671: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 672: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 673: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 674: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 675: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 676: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 677: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 678: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 679: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 680: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 681: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 682: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 683: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 684: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 685: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 686: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 687: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 688: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 689: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 690: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 691: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 692: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 693: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 694: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 695: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 696: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 697: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 698: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 699: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 700: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 701: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 702: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 703: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 704: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 705: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 706: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 707: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 708: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 709: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 710: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 711: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 712: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 713: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 714: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 715: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 716: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 717: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 718: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 719: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 720: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 721: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 722: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 723: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 724: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 725: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 726: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 727: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 728: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 729: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 730: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 731: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 732: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 733: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 734: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 735: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 736: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 737: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 738: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 739: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 740: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 741: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 742: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 743: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 744: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 745: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 746: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 747: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 748: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 749: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 750: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 751: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 752: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 753: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 754: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 755: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 756: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 757: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 758: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 759: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 760: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 761: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 762: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 763: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 764: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 765: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 766: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 767: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 768: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 769: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 770: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 771: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 772: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 773: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 774: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 775: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 776: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 777: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 778: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 779: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 780: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 781: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 782: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 783: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 784: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 785: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 786: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 787: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 788: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 789: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 790: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 791: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 792: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 793: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 794: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 795: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 796: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 797: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 798: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 799: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 800: weight = [[3.35791339]], train loss = [[0.18409348]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 801: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 802: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 803: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 804: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 805: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 806: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 807: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 808: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 809: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 810: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 811: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 812: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 813: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 814: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 815: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 816: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 817: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 818: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 819: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 820: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 821: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 822: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 823: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 824: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 825: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 826: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 827: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 828: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 829: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 830: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 831: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 832: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 833: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 834: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 835: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 836: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 837: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 838: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 839: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 840: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 841: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 842: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 843: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 844: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 845: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 846: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 847: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 848: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 849: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 850: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 851: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 852: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 853: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 854: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 855: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 856: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 857: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 858: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 859: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 860: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 861: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 862: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 863: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 864: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 865: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 866: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 867: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 868: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 869: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 870: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 871: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 872: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 873: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 874: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 875: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 876: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 877: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 878: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 879: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 880: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 881: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 882: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 883: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 884: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 885: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 886: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 887: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 888: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 889: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 890: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 891: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 892: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 893: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 894: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 895: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 896: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 897: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 898: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 899: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 900: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 901: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 902: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 903: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 904: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 905: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 906: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 907: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 908: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 909: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 910: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 911: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 912: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 913: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 914: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 915: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 916: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 917: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 918: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 919: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 920: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 921: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 922: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 923: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 924: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 925: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 926: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 927: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 928: weight = [[3.35791339]], train loss = [[0.18409348]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 929: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 930: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 931: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 932: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 933: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 934: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 935: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 936: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 937: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 938: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 939: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 940: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 941: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 942: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 943: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 944: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 945: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 946: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 947: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 948: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 949: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 950: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 951: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 952: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 953: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 954: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 955: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 956: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 957: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 958: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 959: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 960: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 961: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 962: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 963: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 964: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 965: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 966: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 967: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 968: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 969: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 970: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 971: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 972: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 973: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 974: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 975: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 976: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 977: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 978: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 979: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 980: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 981: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 982: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 983: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 984: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 985: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 986: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 987: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 988: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 989: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 990: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 991: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 992: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 993: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 994: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 995: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 996: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 997: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 998: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 999: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "epoch: 1000: weight = [[3.35791339]], train loss = [[0.18409348]]\n",
      "the time used for optimization: 9.224195003509521\n"
     ]
    }
   ],
   "source": [
    "X_new = 2 * np.random.rand(10000, 1)\n",
    "y_new = 3 * X_new + np.random.rand(10000, 1)\n",
    "\n",
    "eta = 0.1 #Learning rate\n",
    "n_epochs = 1000 #Number of epochs for weight updates\n",
    "d_train = 10000 #Number of training samples\n",
    "n_batches = 20 #Batch size\n",
    "\n",
    "w = np.random.rand(1) #Weight vector\n",
    "\n",
    "t1 = time.time()\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(0, d_train, n_batches):\n",
    "        xi = X_new[i:i + n_batches]\n",
    "        yi = y_new[i:i + n_batches]\n",
    "        gradient = 2 / n_batches * (w.dot(xi.T) - yi.T).dot(xi)\n",
    "        w = w - eta * gradient\n",
    "        train_loss = 1 / n_batches * (w.dot(xi.T) - yi.T).dot((w.dot(xi.T) - yi.T).T)\n",
    "    print('epoch: {}: weight = {}, train loss = {}'.format(epoch + 1, w, train_loss))\n",
    "t2 = time.time()\n",
    "print('the time used for optimization: {}'.format(t2 - t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69762c2a",
   "metadata": {},
   "source": [
    "# Train linear regression model using scikit-learn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "faf74970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.94085204]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 3 * X + np.random.rand(100, 1)\n",
    "\n",
    "LR_model = LinearRegression()\n",
    "LR_model.fit(X, y)\n",
    "print(LR_model.coef_) #Weight vector after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6859d465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X, y, \"b.\")\n",
    "plt.plot(X, LR_model.predict(X), \"r-\")\n",
    "plt.xlabel(\"X\", fontsize=18)\n",
    "plt.ylabel(\"y\", fontsize=14, rotation=0)\n",
    "plt.axis([0, 2, 0, 10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae42f49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.93927939]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 3 * X + np.random.rand(100, 1)\n",
    "\n",
    "sgd_reg = SGDRegressor(max_iter=1000, tol=1e-3, eta0=0.1, random_state=42)\n",
    "sgd_reg.fit(X, y.ravel())\n",
    "print(sgd_reg.coef_) #Weight vector after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5cd76d0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X, y, \"b.\")\n",
    "plt.plot(X, sgd_reg.predict(X), \"r-\")\n",
    "plt.xlabel(\"X\", fontsize=18)\n",
    "plt.ylabel(\"y\", fontsize=14, rotation=0)\n",
    "plt.axis([0, 2, 0, 10])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
